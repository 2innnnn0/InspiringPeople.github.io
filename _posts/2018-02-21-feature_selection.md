---
category: Data Analysis
title: 정규화를 통한 Feature Selection
tags: [R, Feature Selection]

---
----------


### Why Feature Selection
- too many features are not information!!!
- 모델링 결과의 정확도, 예측력도 중요하지만, 데이터 분석 결과를 실제 업무에 적용 시킬 때에는 Feature Selection이 중요한 역할을 함
- 모델링 결과에 해셕력을 더하고 실제 Action으로 활용할 수 있게 함  
- 예) 고장의 주요 원인이 되는 feature를 사전 control, 가격에 영향을 미치는 주요 인자 관리

----------


### Feature Selection
- 특징선택(feature selection)은 분류 정확도를 향상시키기 위해 원본 데이터가 주어졌을 때, 가장 좋은 성능을 보여줄 수 있는 데이터의 부분집합(subset)을 원본 데이터에서 찾아내는 방법
- 노이즈 감소
- 과적합 방지
- 다중공산성 방지
- feature 수 감소로 computation time 줄임

----------


### Feature selection 방법
- subset selection
forward / backward stepwise selection
0~P개를 포함하는 모델 중 AIC, BIC, adjust R2을 참고로 모델 선택
- shrinkage (regularization)
Ridge / Lasso / Elastic Net  
good lambda 값을 찾는 것이 중요, cross validation is good choice
- dimension reduction  

----------  

### bias VS variance
- high bias : stable한 모델, underfitting 유발

![fit1](/assets/img/fit1.png)<br>

- high variance : 모델 복잡도가 높아짐, overfitting 발생

![fit2](/assets/img/fit2.png) 

----------

### Regularization
bias와 variance의 balancing error를 찾기 위한 최적의 feature 수를 찾아가는 과정   
- 트레인 데이터보다 테스트 데이터의 에러 (generalization error)를 줄이는데 목표  
- bias-variance tradeoff에서 variance를 줄이는 방향으로 적용  
- bias를 크게 하지 않으면서 과적합 방지   

![](/assets/img/fit3.png)

----------

### Ridge
- parameter를 축소하여 다중공산성 방지에 쓰임
- 계수 축소에 의해 모델의 복잡도를 줄임
- L2 regularization 활용, 계수의 크기의 제곱값에 대해 penalty 부여

![](/assets/img/ridge.png)

----------

### Lasso
- L1 regularization 활용, 계수의 크기의 절대값과 동일한 penalty 부여
- 변수 선택에 유용

![](/assets/img/lasso.png)

----------

### Lasso & Ridge Pictures
Ridge 모형은 가중치 계수를 한번에 축소시키는데 반해 Lasso 모형은 일부 가중치 계수가 먼저 0으로 수렴
 
왼쪽 : lasso, 오른쪽 : ridge  

![](/assets/img/ridge_lasso_pic.png) 

----------

### Elastic Net
- Ridge : 모든 변수 사용, 계수값은 줄이지만 여전희 모든 변수 보유, 모델 성능 저하 
- Lasso : correlate한 변수 중 단 한 개의 변수만 선택하고 나머지 변수 계수는 0으로 변환, 정보손실로 인한 정확도 저하 
- Elastic Net : Ridge와 Lasso의 Hybrid 방식 (correlate한 다수의 feature 가진 데이터셋에 유리) 
- 계수의 절대값의 함과 제곱합을 동시에 penalty값으로 가지는 모형 
- 상관 변수 그룹을 만들고 Y와 강한 상관 관계 있는 변수 속한 그룹의 변수는 모두 선택 
- 데이터 셋의 크지 않고 feature 개수가 작을 경우 ridge, lasso 보다 성능 안좋을 경우 생김 

----------


## Example

예제 : House Price Prediction in Kaggle  
1. Ridge/Lasso/Enet을 사용하여 feature selection을 하고 선택된 변수를 사용하여 House Price를 예측한다.  
2. Caret Package를 통하여 최적화된 parameter tuning을 하여 Lambda 값을 찾는다.  

----------

Data Prepare


```r
# get the require R packages
library(ggplot2)
library(plyr)
library(dplyr)
library(caret)
library(moments)
library(glmnet)
library(elasticnet)
library(knitr)
library(e1071) # yk

options(width=100)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)


ROOT.DIR <- "./"   #facilitates testing inside and outside of Rmarkdown notebook
```


```r
train <- read.csv(file.path(ROOT.DIR,"input","train.csv"),stringsAsFactors = FALSE)
test <- read.csv(file.path(ROOT.DIR,"input","test.csv"),stringsAsFactors = FALSE)
```


```r
# show the first few rows of training data
head(train)
```

```
##   Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig
## 1  1         60       RL          65    8450   Pave  <NA>      Reg         Lvl    AllPub    Inside
## 2  2         20       RL          80    9600   Pave  <NA>      Reg         Lvl    AllPub       FR2
## 3  3         60       RL          68   11250   Pave  <NA>      IR1         Lvl    AllPub    Inside
## 4  4         70       RL          60    9550   Pave  <NA>      IR1         Lvl    AllPub    Corner
## 5  5         60       RL          84   14260   Pave  <NA>      IR1         Lvl    AllPub       FR2
## 6  6         50       RL          85   14115   Pave  <NA>      IR1         Lvl    AllPub    Inside
##   LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond
## 1       Gtl      CollgCr       Norm       Norm     1Fam     2Story           7           5
## 2       Gtl      Veenker      Feedr       Norm     1Fam     1Story           6           8
## 3       Gtl      CollgCr       Norm       Norm     1Fam     2Story           7           5
## 4       Gtl      Crawfor       Norm       Norm     1Fam     2Story           7           5
## 5       Gtl      NoRidge       Norm       Norm     1Fam     2Story           8           5
## 6       Gtl      Mitchel       Norm       Norm     1Fam     1.5Fin           5           5
##   YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual
## 1      2003         2003     Gable  CompShg     VinylSd     VinylSd    BrkFace        196        Gd
## 2      1976         1976     Gable  CompShg     MetalSd     MetalSd       None          0        TA
## 3      2001         2002     Gable  CompShg     VinylSd     VinylSd    BrkFace        162        Gd
## 4      1915         1970     Gable  CompShg     Wd Sdng     Wd Shng       None          0        TA
## 5      2000         2000     Gable  CompShg     VinylSd     VinylSd    BrkFace        350        Gd
## 6      1993         1995     Gable  CompShg     VinylSd     VinylSd       None          0        TA
##   ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2
## 1        TA      PConc       Gd       TA           No          GLQ        706          Unf
## 2        TA     CBlock       Gd       TA           Gd          ALQ        978          Unf
## 3        TA      PConc       Gd       TA           Mn          GLQ        486          Unf
## 4        TA     BrkTil       TA       Gd           No          ALQ        216          Unf
## 5        TA      PConc       Gd       TA           Av          GLQ        655          Unf
## 6        TA       Wood       Gd       TA           No          GLQ        732          Unf
##   BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating HeatingQC CentralAir Electrical X1stFlrSF X2ndFlrSF
## 1          0       150         856    GasA        Ex          Y      SBrkr       856       854
## 2          0       284        1262    GasA        Ex          Y      SBrkr      1262         0
## 3          0       434         920    GasA        Ex          Y      SBrkr       920       866
## 4          0       540         756    GasA        Gd          Y      SBrkr       961       756
## 5          0       490        1145    GasA        Ex          Y      SBrkr      1145      1053
## 6          0        64         796    GasA        Ex          Y      SBrkr       796       566
##   LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr
## 1            0      1710            1            0        2        1            3            1
## 2            0      1262            0            1        2        0            3            1
## 3            0      1786            1            0        2        1            3            1
## 4            0      1717            1            0        1        0            3            1
## 5            0      2198            1            0        2        1            4            1
## 6            0      1362            1            0        1        1            1            1
##   KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish
## 1          Gd            8        Typ          0        <NA>     Attchd        2003          RFn
## 2          TA            6        Typ          1          TA     Attchd        1976          RFn
## 3          Gd            6        Typ          1          TA     Attchd        2001          RFn
## 4          Gd            7        Typ          1          Gd     Detchd        1998          Unf
## 5          Gd            9        Typ          1          TA     Attchd        2000          RFn
## 6          TA            5        Typ          0        <NA>     Attchd        1993          Unf
##   GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch
## 1          2        548         TA         TA          Y          0          61             0
## 2          2        460         TA         TA          Y        298           0             0
## 3          2        608         TA         TA          Y          0          42             0
## 4          3        642         TA         TA          Y          0          35           272
## 5          3        836         TA         TA          Y        192          84             0
## 6          2        480         TA         TA          Y         40          30             0
##   X3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType
## 1          0           0        0   <NA>  <NA>        <NA>       0      2   2008       WD
## 2          0           0        0   <NA>  <NA>        <NA>       0      5   2007       WD
## 3          0           0        0   <NA>  <NA>        <NA>       0      9   2008       WD
## 4          0           0        0   <NA>  <NA>        <NA>       0      2   2006       WD
## 5          0           0        0   <NA>  <NA>        <NA>       0     12   2008       WD
## 6        320           0        0   <NA> MnPrv        Shed     700     10   2009       WD
##   SaleCondition SalePrice
## 1        Normal    208500
## 2        Normal    181500
## 3        Normal    223500
## 4       Abnorml    140000
## 5        Normal    250000
## 6        Normal    143000
```

```r
# show dimension of training data sample
dim(train)
```

```
## [1] 1460   81
```


```r
# combine train and test data for preprocessing
all_data <- rbind(select(train,MSSubClass:SaleCondition),
                  select(test,MSSubClass:SaleCondition))
```



----------  

### Data preprocessing:
편향된 Y 변수값을 log 지수로 변환  

```r
# get data frame of SalePrice and log(SalePrice + 1) for plotting
df <- rbind(data.frame(version="log(price+1)",x=log(train$SalePrice + 1)),
            data.frame(version="price",x=train$SalePrice))

# plot histogram
ggplot(data=df) +
  facet_wrap(~version,ncol=2,scales="free_x") +
  geom_histogram(aes(x=x))
```
![](/assets/img/In_5-1.png)  

numeric 변수 중 편향 분포를 가진 변수는 log 지수화  

```r
# transform SalePrice target to log form
train$SalePrice <- log(train$SalePrice + 1)

# for numeric feature with excessive skewness, perform log transformation
# first get data type for each feature
feature_classes <- sapply(names(all_data),function(x){class(all_data[[x]])})
numeric_feats <-names(feature_classes[feature_classes != "character"])

# determine skew for each numeric feature
skewed_feats <- sapply(numeric_feats,function(x){skewness(all_data[[x]],na.rm=TRUE)})

# keep only features that exceed a threshold for skewness
skewed_feats <- skewed_feats[skewed_feats > 0.75]

# transform excessively skewed features with log(x + 1)
for(x in names(skewed_feats)) {
  all_data[[x]] <- log(all_data[[x]] + 1)
}
```


caegorical feature는 dummy variable로 변환 

```r
# get names of categorical features
categorical_feats <- names(feature_classes[feature_classes == "character"])

# use caret dummyVars function for hot one encoding for categorical features
dummies <- dummyVars(~.,all_data[categorical_feats])
categorical_1_hot <- predict(dummies,all_data[categorical_feats])
categorical_1_hot[is.na(categorical_1_hot)] <- 0  #for any level that was NA, set to zero
```


numeric 변수 중 NA값은 mean값으로 치환

```r
# for any missing values in numeric features, impute mean of that feature
numeric_df <- all_data[numeric_feats]

for (x in numeric_feats) {
    mean_value <- mean(train[[x]],na.rm = TRUE)
    all_data[[x]][is.na(all_data[[x]])] <- mean_value
}
```

numeric/categorical 전처리 데이터 병합, train/test data 분리

```r
# reconstruct all_data with pre-processed data
all_data <- cbind(all_data[numeric_feats],categorical_1_hot)

# create data for training and test
X_train <- all_data[1:nrow(train),]
X_test <- all_data[(nrow(train)+1):nrow(all_data),]
y <- train$SalePrice
```
----------

### Models  
Caret Package 사용  
Ridge / Lasso / Elstic Net에서 최적의 람다값을 찾는 것이 중요    


```r
# set up caret model training parameters
# model specific training parameter
CARET.TRAIN.CTRL <- trainControl(method="repeatedcv",
                                 number=5,
                                 repeats=5,
                                 verboseIter=FALSE)
```

----------  

#### Ridge  

```r
# test out Ridge regression model

lambdas <- seq(1,0,-0.001)

# train model
set.seed(123)  # for reproducibility
model_ridge <- train(x=X_train,y=y,
                  method="glmnet",
                  metric="RMSE",
                  maximize=FALSE,
                  trControl=CARET.TRAIN.CTRL,
                  tuneGrid=expand.grid(alpha=0, # Ridge regression
                                       lambda=lambdas))

model_ridge
```

```
## glmnet 
## 
## 1460 samples
##  288 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 5 times) 
## Summary of sample sizes: 1169, 1168, 1167, 1167, 1169, 1168, ... 
## Resampling results across tuning parameters:
## 
##   lambda  RMSE       Rsquared   MAE       
##   0.000   0.1320491  0.8908273  0.08516620
##   0.001   0.1320491  0.8908273  0.08516620
##   0.002   0.1320491  0.8908273  0.08516620
##   0.003   0.1320491  0.8908273  0.08516620
##   0.004   0.1320491  0.8908273  0.08516620
##   0.005   0.1320491  0.8908273  0.08516620
##   0.006   0.1320491  0.8908273  0.08516620
##   0.007   0.1320491  0.8908273  0.08516620
##   0.008   0.1320491  0.8908273  0.08516620
##   0.009   0.1320491  0.8908273  0.08516620
##   0.010   0.1320491  0.8908273  0.08516620
##   0.011   0.1320491  0.8908273  0.08516620
##   0.012   0.1320491  0.8908273  0.08516620
##   0.013   0.1320491  0.8908273  0.08516620
##   0.014   0.1320491  0.8908273  0.08516620
##   0.015   0.1320491  0.8908273  0.08516620
##   0.016   0.1320491  0.8908273  0.08516620
##   0.017   0.1320491  0.8908273  0.08516620
##   0.018   0.1320491  0.8908273  0.08516620
##   0.019   0.1320491  0.8908273  0.08516620
##   0.020   0.1320491  0.8908273  0.08516620
##   0.021   0.1320491  0.8908273  0.08516620
##   0.022   0.1320491  0.8908273  0.08516620
##   0.023   0.1320491  0.8908273  0.08516620
##   0.024   0.1320491  0.8908273  0.08516620
##   0.025   0.1320491  0.8908273  0.08516620
##   0.026   0.1320491  0.8908273  0.08516620
##   0.027   0.1320491  0.8908273  0.08516620
##   0.028   0.1320491  0.8908273  0.08516620
##   0.029   0.1320491  0.8908273  0.08516620
##   0.030   0.1320491  0.8908273  0.08516620
##   0.031   0.1320491  0.8908273  0.08516620
##   0.032   0.1320489  0.8908275  0.08516614
##   0.033   0.1320263  0.8908594  0.08516397
##   0.034   0.1319730  0.8909330  0.08516034
##   0.035   0.1319209  0.8910051  0.08515795
##   0.036   0.1318712  0.8910741  0.08515680
##   0.037   0.1318238  0.8911398  0.08515431
##   0.038   0.1317781  0.8912033  0.08515410
##   0.039   0.1317342  0.8912643  0.08515589
##   0.040   0.1316918  0.8913234  0.08515781
##   0.041   0.1316508  0.8913807  0.08516013
##   0.042   0.1316114  0.8914359  0.08516356
##   0.043   0.1315738  0.8914887  0.08516839
##   0.044   0.1315381  0.8915388  0.08517297
##   0.045   0.1315037  0.8915874  0.08517817
##   0.046   0.1314706  0.8916342  0.08518402
##   0.047   0.1314389  0.8916791  0.08519065
##   0.048   0.1314088  0.8917220  0.08519770
##   0.049   0.1313796  0.8917638  0.08520482
##   0.050   0.1313516  0.8918041  0.08521396
##   0.051   0.1313248  0.8918428  0.08522417
##   0.052   0.1312993  0.8918798  0.08523510
##   0.053   0.1312746  0.8919159  0.08524520
##   0.054   0.1312509  0.8919509  0.08525511
##   0.055   0.1312282  0.8919845  0.08526583
##   0.056   0.1312065  0.8920167  0.08527829
##   0.057   0.1311859  0.8920475  0.08529106
##   0.058   0.1311656  0.8920778  0.08530303
##   0.059   0.1311462  0.8921070  0.08531516
##   0.060   0.1311278  0.8921350  0.08532802
##   0.061   0.1311102  0.8921618  0.08534132
##   0.062   0.1310937  0.8921873  0.08535491
##   0.063   0.1310784  0.8922112  0.08536898
##   0.064   0.1310634  0.8922348  0.08538211
##   0.065   0.1310491  0.8922576  0.08539563
##   0.066   0.1310357  0.8922793  0.08540975
##   0.067   0.1310230  0.8922999  0.08542437
##   0.068   0.1310112  0.8923195  0.08543961
##   0.069   0.1310000  0.8923383  0.08545509
##   0.070   0.1309893  0.8923567  0.08547001
##   0.071   0.1309790  0.8923746  0.08548514
##   0.072   0.1309694  0.8923915  0.08550067
##   0.073   0.1309606  0.8924075  0.08551649
##   0.074   0.1309526  0.8924226  0.08553311
##   0.075   0.1309454  0.8924365  0.08555002
##   0.076   0.1309389  0.8924496  0.08556678
##   0.077   0.1309327  0.8924625  0.08558289
##   0.078   0.1309269  0.8924749  0.08559959
##   0.079   0.1309218  0.8924865  0.08561676
##   0.080   0.1309173  0.8924972  0.08563423
##   0.081   0.1309135  0.8925071  0.08565212
##   0.082   0.1309103  0.8925162  0.08567071
##   0.083   0.1309074  0.8925249  0.08568898
##   0.084   0.1309044  0.8925339  0.08570629
##   0.085   0.1309016  0.8925426  0.08572350
##   0.086   0.1308995  0.8925505  0.08574111
##   0.087   0.1308979  0.8925577  0.08575898
##   0.088   0.1308969  0.8925642  0.08577733
##   0.089   0.1308965  0.8925700  0.08579619
##   0.090   0.1308966  0.8925751  0.08581488
##   0.091   0.1308971  0.8925797  0.08583318
##   0.092   0.1308975  0.8925845  0.08585075
##   0.093   0.1308978  0.8925895  0.08586820
##   0.094   0.1308987  0.8925939  0.08588603
##   0.095   0.1309000  0.8925977  0.08590431
##   0.096   0.1309019  0.8926008  0.08592318
##   0.097   0.1309042  0.8926034  0.08594239
##   0.098   0.1309070  0.8926053  0.08596163
##   0.099   0.1309102  0.8926067  0.08598093
##   0.100   0.1309139  0.8926076  0.08600014
##   0.101   0.1309175  0.8926085  0.08601847
##   0.102   0.1309211  0.8926097  0.08603637
##   0.103   0.1309252  0.8926104  0.08605470
##   0.104   0.1309296  0.8926105  0.08607335
##   0.105   0.1309345  0.8926101  0.08609246
##   0.106   0.1309399  0.8926092  0.08611169
##   0.107   0.1309457  0.8926077  0.08613106
##   0.108   0.1309518  0.8926058  0.08615031
##   0.109   0.1309583  0.8926035  0.08616931
##   0.110   0.1309650  0.8926010  0.08618826
##   0.111   0.1309716  0.8925988  0.08620698
##   0.112   0.1309783  0.8925965  0.08622570
##   0.113   0.1309853  0.8925939  0.08624475
##   0.114   0.1309928  0.8925907  0.08626391
##   0.115   0.1310006  0.8925871  0.08628336
##   0.116   0.1310089  0.8925830  0.08630323
##   0.117   0.1310175  0.8925784  0.08632357
##   0.118   0.1310265  0.8925735  0.08634407
##   0.119   0.1310358  0.8925680  0.08636471
##   0.120   0.1310454  0.8925622  0.08638540
##   0.121   0.1310548  0.8925567  0.08640544
##   0.122   0.1310640  0.8925516  0.08642531
##   0.123   0.1310733  0.8925464  0.08644533
##   0.124   0.1310830  0.8925409  0.08646550
##   0.125   0.1310929  0.8925350  0.08648598
##   0.126   0.1311032  0.8925286  0.08650676
##   0.127   0.1311138  0.8925218  0.08652765
##   0.128   0.1311248  0.8925147  0.08654896
##   0.129   0.1311361  0.8925071  0.08657059
##   0.130   0.1311476  0.8924992  0.08659226
##   0.131   0.1311594  0.8924911  0.08661426
##   0.132   0.1311713  0.8924829  0.08663610
##   0.133   0.1311830  0.8924750  0.08665739
##   0.134   0.1311946  0.8924673  0.08667843
##   0.135   0.1312063  0.8924595  0.08669962
##   0.136   0.1312183  0.8924514  0.08672104
##   0.137   0.1312306  0.8924428  0.08674256
##   0.138   0.1312432  0.8924340  0.08676414
##   0.139   0.1312561  0.8924247  0.08678582
##   0.140   0.1312693  0.8924151  0.08680763
##   0.141   0.1312828  0.8924052  0.08682973
##   0.142   0.1312965  0.8923950  0.08685181
##   0.143   0.1313104  0.8923845  0.08687403
##   0.144   0.1313245  0.8923738  0.08689605
##   0.145   0.1313387  0.8923631  0.08691795
##   0.146   0.1313526  0.8923528  0.08693912
##   0.147   0.1313662  0.8923429  0.08696011
##   0.148   0.1313799  0.8923329  0.08698119
##   0.149   0.1313939  0.8923226  0.08700246
##   0.150   0.1314081  0.8923120  0.08702388
##   0.151   0.1314226  0.8923011  0.08704557
##   0.152   0.1314374  0.8922898  0.08706754
##   0.153   0.1314524  0.8922783  0.08708966
##   0.154   0.1314677  0.8922664  0.08711182
##   0.155   0.1314832  0.8922543  0.08713409
##   0.156   0.1314989  0.8922420  0.08715638
##   0.157   0.1315148  0.8922295  0.08717882
##   0.158   0.1315309  0.8922167  0.08720112
##   0.159   0.1315470  0.8922039  0.08722324
##   0.160   0.1315629  0.8921916  0.08724489
##   0.161   0.1315786  0.8921796  0.08726631
##   0.162   0.1315941  0.8921678  0.08728745
##   0.163   0.1316099  0.8921559  0.08730862
##   0.164   0.1316259  0.8921436  0.08732993
##   0.165   0.1316421  0.8921311  0.08735143
##   0.166   0.1316586  0.8921184  0.08737301
##   0.167   0.1316752  0.8921054  0.08739489
##   0.168   0.1316922  0.8920921  0.08741698
##   0.169   0.1317093  0.8920785  0.08743923
##   0.170   0.1317267  0.8920647  0.08746163
##   0.171   0.1317442  0.8920507  0.08748411
##   0.172   0.1317618  0.8920366  0.08750661
##   0.173   0.1317797  0.8920222  0.08752909
##   0.174   0.1317976  0.8920078  0.08755138
##   0.175   0.1318155  0.8919935  0.08757336
##   0.176   0.1318331  0.8919796  0.08759492
##   0.177   0.1318507  0.8919659  0.08761629
##   0.178   0.1318683  0.8919522  0.08763758
##   0.179   0.1318861  0.8919383  0.08765905
##   0.180   0.1319041  0.8919242  0.08768061
##   0.181   0.1319223  0.8919099  0.08770228
##   0.182   0.1319406  0.8918953  0.08772413
##   0.183   0.1319592  0.8918805  0.08774626
##   0.184   0.1319780  0.8918655  0.08776854
##   0.185   0.1319970  0.8918502  0.08779093
##   0.186   0.1320162  0.8918347  0.08781343
##   0.187   0.1320355  0.8918190  0.08783605
##   0.188   0.1320550  0.8918032  0.08785859
##   0.189   0.1320745  0.8917872  0.08788121
##   0.190   0.1320942  0.8917711  0.08790379
##   0.191   0.1321140  0.8917550  0.08792633
##   0.192   0.1321335  0.8917391  0.08794851
##   0.193   0.1321528  0.8917235  0.08797028
##   0.194   0.1321719  0.8917082  0.08799185
##   0.195   0.1321911  0.8916930  0.08801346
##   0.196   0.1322103  0.8916776  0.08803517
##   0.197   0.1322298  0.8916619  0.08805700
##   0.198   0.1322494  0.8916461  0.08807892
##   0.199   0.1322693  0.8916301  0.08810089
##   0.200   0.1322892  0.8916139  0.08812300
##   0.201   0.1323094  0.8915975  0.08814522
##   0.202   0.1323297  0.8915809  0.08816755
##   0.203   0.1323502  0.8915641  0.08818998
##   0.204   0.1323709  0.8915470  0.08821257
##   0.205   0.1323918  0.8915298  0.08823518
##   0.206   0.1324127  0.8915125  0.08825776
##   0.207   0.1324337  0.8914951  0.08828035
##   0.208   0.1324548  0.8914776  0.08830292
##   0.209   0.1324761  0.8914600  0.08832531
##   0.210   0.1324972  0.8914425  0.08834760
##   0.211   0.1325182  0.8914253  0.08836958
##   0.212   0.1325389  0.8914085  0.08839129
##   0.213   0.1325594  0.8913919  0.08841298
##   0.214   0.1325800  0.8913754  0.08843469
##   0.215   0.1326006  0.8913587  0.08845647
##   0.216   0.1326214  0.8913419  0.08847829
##   0.217   0.1326424  0.8913249  0.08850019
##   0.218   0.1326635  0.8913077  0.08852226
##   0.219   0.1326848  0.8912904  0.08854440
##   0.220   0.1327062  0.8912729  0.08856673
##   0.221   0.1327278  0.8912552  0.08858913
##   0.222   0.1327495  0.8912373  0.08861166
##   0.223   0.1327714  0.8912192  0.08863429
##   0.224   0.1327935  0.8912010  0.08865701
##   0.225   0.1328156  0.8911826  0.08867982
##   0.226   0.1328379  0.8911642  0.08870265
##   0.227   0.1328602  0.8911457  0.08872550
##   0.228   0.1328826  0.8911271  0.08874837
##   0.229   0.1329051  0.8911083  0.08877110
##   0.230   0.1329276  0.8910897  0.08879370
##   0.231   0.1329499  0.8910713  0.08881597
##   0.232   0.1329719  0.8910532  0.08883786
##   0.233   0.1329938  0.8910354  0.08885966
##   0.234   0.1330156  0.8910178  0.08888141
##   0.235   0.1330375  0.8910002  0.08890311
##   0.236   0.1330594  0.8909825  0.08892490
##   0.237   0.1330815  0.8909646  0.08894681
##   0.238   0.1331037  0.8909466  0.08896872
##   0.239   0.1331260  0.8909284  0.08899065
##   0.240   0.1331485  0.8909101  0.08901265
##   0.241   0.1331711  0.8908917  0.08903475
##   0.242   0.1331939  0.8908730  0.08905697
##   0.243   0.1332168  0.8908543  0.08907924
##   0.244   0.1332398  0.8908353  0.08910157
##   0.245   0.1332629  0.8908163  0.08912402
##   0.246   0.1332862  0.8907970  0.08914660
##   0.247   0.1333096  0.8907777  0.08916925
##   0.248   0.1333330  0.8907583  0.08919194
##   0.249   0.1333565  0.8907389  0.08921464
##   0.250   0.1333801  0.8907193  0.08923747
##   0.251   0.1334037  0.8906997  0.08926018
##   0.252   0.1334274  0.8906801  0.08928286
##   0.253   0.1334510  0.8906606  0.08930543
##   0.254   0.1334744  0.8906414  0.08932774
##   0.255   0.1334976  0.8906225  0.08934974
##   0.256   0.1335206  0.8906038  0.08937166
##   0.257   0.1335436  0.8905853  0.08939343
##   0.258   0.1335666  0.8905667  0.08941516
##   0.259   0.1335897  0.8905480  0.08943697
##   0.260   0.1336130  0.8905293  0.08945882
##   0.261   0.1336364  0.8905103  0.08948068
##   0.262   0.1336598  0.8904913  0.08950254
##   0.263   0.1336834  0.8904721  0.08952450
##   0.264   0.1337071  0.8904528  0.08954652
##   0.265   0.1337310  0.8904333  0.08956854
##   0.266   0.1337549  0.8904137  0.08959063
##   0.267   0.1337790  0.8903940  0.08961277
##   0.268   0.1338032  0.8903742  0.08963493
##   0.269   0.1338275  0.8903542  0.08965717
##   0.270   0.1338519  0.8903341  0.08967945
##   0.271   0.1338764  0.8903139  0.08970178
##   0.272   0.1339010  0.8902936  0.08972405
##   0.273   0.1339255  0.8902734  0.08974625
##   0.274   0.1339502  0.8902530  0.08976845
##   0.275   0.1339749  0.8902326  0.08979053
##   0.276   0.1339996  0.8902122  0.08981249
##   0.277   0.1340244  0.8901918  0.08983440
##   0.278   0.1340490  0.8901715  0.08985625
##   0.279   0.1340734  0.8901515  0.08987786
##   0.280   0.1340976  0.8901318  0.08989924
##   0.281   0.1341217  0.8901122  0.08992051
##   0.282   0.1341457  0.8900929  0.08994175
##   0.283   0.1341698  0.8900735  0.08996303
##   0.284   0.1341940  0.8900540  0.08998430
##   0.285   0.1342183  0.8900344  0.09000563
##   0.286   0.1342426  0.8900147  0.09002702
##   0.287   0.1342671  0.8899949  0.09004849
##   0.288   0.1342917  0.8899750  0.09007005
##   0.289   0.1343163  0.8899549  0.09009176
##   0.290   0.1343411  0.8899348  0.09011352
##   0.291   0.1343660  0.8899145  0.09013538
##   0.292   0.1343910  0.8898941  0.09015741
##   0.293   0.1344160  0.8898736  0.09017955
##   0.294   0.1344412  0.8898530  0.09020187
##   0.295   0.1344665  0.8898323  0.09022426
##   0.296   0.1344919  0.8898114  0.09024670
##   0.297   0.1345174  0.8897905  0.09026921
##   0.298   0.1345429  0.8897694  0.09029179
##   0.299   0.1345685  0.8897484  0.09031431
##   0.300   0.1345940  0.8897274  0.09033686
##   0.301   0.1346197  0.8897063  0.09035945
##   0.302   0.1346454  0.8896852  0.09038200
##   0.303   0.1346711  0.8896640  0.09040455
##   0.304   0.1346968  0.8896429  0.09042704
##   0.305   0.1347223  0.8896220  0.09044950
##   0.306   0.1347476  0.8896014  0.09047169
##   0.307   0.1347728  0.8895811  0.09049374
##   0.308   0.1347978  0.8895609  0.09051573
##   0.309   0.1348228  0.8895409  0.09053769
##   0.310   0.1348477  0.8895209  0.09055961
##   0.311   0.1348727  0.8895009  0.09058157
##   0.312   0.1348978  0.8894809  0.09060355
##   0.313   0.1349230  0.8894607  0.09062559
##   0.314   0.1349482  0.8894404  0.09064770
##   0.315   0.1349736  0.8894200  0.09066989
##   0.316   0.1349990  0.8893995  0.09069211
##   0.317   0.1350246  0.8893789  0.09071434
##   0.318   0.1350502  0.8893583  0.09073661
##   0.319   0.1350759  0.8893375  0.09075897
##   0.320   0.1351017  0.8893166  0.09078146
##   0.321   0.1351276  0.8892956  0.09080403
##   0.322   0.1351535  0.8892746  0.09082664
##   0.323   0.1351796  0.8892534  0.09084930
##   0.324   0.1352057  0.8892321  0.09087198
##   0.325   0.1352320  0.8892107  0.09089470
##   0.326   0.1352583  0.8891892  0.09091749
##   0.327   0.1352847  0.8891677  0.09094035
##   0.328   0.1353110  0.8891462  0.09096314
##   0.329   0.1353374  0.8891247  0.09098594
##   0.330   0.1353639  0.8891031  0.09100871
##   0.331   0.1353904  0.8890815  0.09103148
##   0.332   0.1354169  0.8890599  0.09105416
##   0.333   0.1354434  0.8890383  0.09107684
##   0.334   0.1354698  0.8890168  0.09109953
##   0.335   0.1354960  0.8889955  0.09112210
##   0.336   0.1355221  0.8889745  0.09114438
##   0.337   0.1355480  0.8889538  0.09116645
##   0.338   0.1355738  0.8889332  0.09118843
##   0.339   0.1355995  0.8889127  0.09121036
##   0.340   0.1356252  0.8888923  0.09123226
##   0.341   0.1356510  0.8888720  0.09125416
##   0.342   0.1356768  0.8888515  0.09127608
##   0.343   0.1357027  0.8888309  0.09129801
##   0.344   0.1357286  0.8888103  0.09132000
##   0.345   0.1357547  0.8887895  0.09134217
##   0.346   0.1357808  0.8887687  0.09136439
##   0.347   0.1358070  0.8887478  0.09138667
##   0.348   0.1358333  0.8887268  0.09140894
##   0.349   0.1358596  0.8887057  0.09143124
##   0.350   0.1358860  0.8886846  0.09145360
##   0.351   0.1359125  0.8886633  0.09147601
##   0.352   0.1359391  0.8886420  0.09149847
##   0.353   0.1359658  0.8886206  0.09152100
##   0.354   0.1359925  0.8885990  0.09154368
##   0.355   0.1360193  0.8885775  0.09156652
##   0.356   0.1360462  0.8885558  0.09158951
##   0.357   0.1360732  0.8885340  0.09161259
##   0.358   0.1361002  0.8885121  0.09163573
##   0.359   0.1361273  0.8884902  0.09165887
##   0.360   0.1361544  0.8884684  0.09168193
##   0.361   0.1361814  0.8884466  0.09170498
##   0.362   0.1362086  0.8884246  0.09172806
##   0.363   0.1362358  0.8884027  0.09175117
##   0.364   0.1362629  0.8883808  0.09177424
##   0.365   0.1362901  0.8883588  0.09179727
##   0.366   0.1363172  0.8883370  0.09182031
##   0.367   0.1363442  0.8883152  0.09184326
##   0.368   0.1363711  0.8882937  0.09186601
##   0.369   0.1363978  0.8882725  0.09188856
##   0.370   0.1364243  0.8882514  0.09191096
##   0.371   0.1364508  0.8882305  0.09193326
##   0.372   0.1364772  0.8882097  0.09195560
##   0.373   0.1365035  0.8881890  0.09197785
##   0.374   0.1365299  0.8881683  0.09200010
##   0.375   0.1365564  0.8881475  0.09202237
##   0.376   0.1365829  0.8881267  0.09204464
##   0.377   0.1366095  0.8881057  0.09206698
##   0.378   0.1366361  0.8880847  0.09208941
##   0.379   0.1366628  0.8880637  0.09211193
##   0.380   0.1366896  0.8880425  0.09213454
##   0.381   0.1367165  0.8880213  0.09215721
##   0.382   0.1367434  0.8880000  0.09217990
##   0.383   0.1367704  0.8879786  0.09220265
##   0.384   0.1367974  0.8879572  0.09222550
##   0.385   0.1368245  0.8879356  0.09224842
##   0.386   0.1368517  0.8879140  0.09227139
##   0.387   0.1368790  0.8878924  0.09229438
##   0.388   0.1369063  0.8878706  0.09231743
##   0.389   0.1369337  0.8878488  0.09234054
##   0.390   0.1369612  0.8878269  0.09236372
##   0.391   0.1369887  0.8878049  0.09238695
##   0.392   0.1370163  0.8877829  0.09241029
##   0.393   0.1370440  0.8877607  0.09243367
##   0.394   0.1370717  0.8877386  0.09245705
##   0.395   0.1370993  0.8877165  0.09248040
##   0.396   0.1371269  0.8876944  0.09250377
##   0.397   0.1371547  0.8876722  0.09252715
##   0.398   0.1371824  0.8876500  0.09255060
##   0.399   0.1372102  0.8876279  0.09257398
##   0.400   0.1372379  0.8876057  0.09259728
##   0.401   0.1372656  0.8875836  0.09262052
##   0.402   0.1372933  0.8875615  0.09264374
##   0.403   0.1373209  0.8875396  0.09266683
##   0.404   0.1373483  0.8875179  0.09268966
##   0.405   0.1373755  0.8874965  0.09271235
##   0.406   0.1374026  0.8874752  0.09273493
##   0.407   0.1374296  0.8874541  0.09275743
##   0.408   0.1374566  0.8874331  0.09277991
##   0.409   0.1374835  0.8874122  0.09280232
##   0.410   0.1375104  0.8873912  0.09282471
##   0.411   0.1375373  0.8873703  0.09284709
##   0.412   0.1375644  0.8873493  0.09286947
##   0.413   0.1375914  0.8873282  0.09289186
##   0.414   0.1376186  0.8873071  0.09291427
##   0.415   0.1376458  0.8872858  0.09293672
##   0.416   0.1376731  0.8872646  0.09295925
##   0.417   0.1377004  0.8872432  0.09298181
##   0.418   0.1377277  0.8872218  0.09300440
##   0.419   0.1377552  0.8872004  0.09302698
##   0.420   0.1377827  0.8871788  0.09304959
##   0.421   0.1378102  0.8871572  0.09307224
##   0.422   0.1378378  0.8871356  0.09309490
##   0.423   0.1378655  0.8871138  0.09311761
##   0.424   0.1378932  0.8870920  0.09314040
##   0.425   0.1379210  0.8870702  0.09316326
##   0.426   0.1379489  0.8870483  0.09318619
##   0.427   0.1379768  0.8870263  0.09320920
##   0.428   0.1380048  0.8870042  0.09323225
##   0.429   0.1380328  0.8869821  0.09325532
##   0.430   0.1380609  0.8869599  0.09327842
##   0.431   0.1380890  0.8869376  0.09330160
##   0.432   0.1381172  0.8869153  0.09332483
##   0.433   0.1381454  0.8868931  0.09334800
##   0.434   0.1381735  0.8868709  0.09337112
##   0.435   0.1382016  0.8868486  0.09339433
##   0.436   0.1382299  0.8868263  0.09341753
##   0.437   0.1382581  0.8868039  0.09344072
##   0.438   0.1382863  0.8867817  0.09346377
##   0.439   0.1383145  0.8867594  0.09348679
##   0.440   0.1383427  0.8867372  0.09350978
##   0.441   0.1383709  0.8867150  0.09353282
##   0.442   0.1383989  0.8866929  0.09355572
##   0.443   0.1384268  0.8866711  0.09357840
##   0.444   0.1384545  0.8866494  0.09360092
##   0.445   0.1384821  0.8866280  0.09362325
##   0.446   0.1385096  0.8866067  0.09364555
##   0.447   0.1385371  0.8865855  0.09366783
##   0.448   0.1385645  0.8865644  0.09369013
##   0.449   0.1385918  0.8865434  0.09371240
##   0.450   0.1386192  0.8865224  0.09373467
##   0.451   0.1386466  0.8865013  0.09375700
##   0.452   0.1386740  0.8864802  0.09377932
##   0.453   0.1387016  0.8864591  0.09380164
##   0.454   0.1387291  0.8864379  0.09382401
##   0.455   0.1387567  0.8864166  0.09384644
##   0.456   0.1387844  0.8863953  0.09386890
##   0.457   0.1388121  0.8863739  0.09389141
##   0.458   0.1388399  0.8863524  0.09391392
##   0.459   0.1388677  0.8863309  0.09393644
##   0.460   0.1388956  0.8863094  0.09395897
##   0.461   0.1389235  0.8862878  0.09398156
##   0.462   0.1389514  0.8862661  0.09400418
##   0.463   0.1389795  0.8862444  0.09402679
##   0.464   0.1390075  0.8862226  0.09404944
##   0.465   0.1390357  0.8862008  0.09407212
##   0.466   0.1390638  0.8861789  0.09409482
##   0.467   0.1390921  0.8861569  0.09411755
##   0.468   0.1391203  0.8861349  0.09414030
##   0.469   0.1391487  0.8861128  0.09416307
##   0.470   0.1391770  0.8860907  0.09418587
##   0.471   0.1392055  0.8860685  0.09420872
##   0.472   0.1392340  0.8860463  0.09423164
##   0.473   0.1392625  0.8860240  0.09425457
##   0.474   0.1392911  0.8860016  0.09427754
##   0.475   0.1393196  0.8859793  0.09430048
##   0.476   0.1393481  0.8859571  0.09432335
##   0.477   0.1393766  0.8859348  0.09434626
##   0.478   0.1394052  0.8859125  0.09436919
##   0.479   0.1394338  0.8858901  0.09439213
##   0.480   0.1394625  0.8858678  0.09441509
##   0.481   0.1394910  0.8858455  0.09443793
##   0.482   0.1395196  0.8858232  0.09446080
##   0.483   0.1395481  0.8858010  0.09448368
##   0.484   0.1395766  0.8857788  0.09450655
##   0.485   0.1396050  0.8857568  0.09452933
##   0.486   0.1396333  0.8857349  0.09455200
##   0.487   0.1396614  0.8857132  0.09457454
##   0.488   0.1396894  0.8856917  0.09459691
##   0.489   0.1397173  0.8856703  0.09461924
##   0.490   0.1397451  0.8856491  0.09464153
##   0.491   0.1397728  0.8856279  0.09466388
##   0.492   0.1398006  0.8856069  0.09468615
##   0.493   0.1398283  0.8855859  0.09470840
##   0.494   0.1398560  0.8855649  0.09473060
##   0.495   0.1398837  0.8855438  0.09475279
##   0.496   0.1399115  0.8855227  0.09477501
##   0.497   0.1399393  0.8855016  0.09479724
##   0.498   0.1399672  0.8854804  0.09481955
##   0.499   0.1399952  0.8854592  0.09484192
##   0.500   0.1400231  0.8854379  0.09486435
##   0.501   0.1400511  0.8854165  0.09488682
##   0.502   0.1400792  0.8853951  0.09490928
##   0.503   0.1401073  0.8853737  0.09493176
##   0.504   0.1401355  0.8853522  0.09495427
##   0.505   0.1401636  0.8853307  0.09497679
##   0.506   0.1401919  0.8853091  0.09499938
##   0.507   0.1402202  0.8852875  0.09502197
##   0.508   0.1402485  0.8852658  0.09504463
##   0.509   0.1402769  0.8852440  0.09506729
##   0.510   0.1403053  0.8852222  0.09508998
##   0.511   0.1403337  0.8852004  0.09511274
##   0.512   0.1403623  0.8851785  0.09513555
##   0.513   0.1403908  0.8851566  0.09515838
##   0.514   0.1404194  0.8851346  0.09518130
##   0.515   0.1404480  0.8851126  0.09520429
##   0.516   0.1404767  0.8850905  0.09522734
##   0.517   0.1405054  0.8850684  0.09525043
##   0.518   0.1405342  0.8850462  0.09527353
##   0.519   0.1405630  0.8850239  0.09529669
##   0.520   0.1405919  0.8850016  0.09531986
##   0.521   0.1406207  0.8849794  0.09534302
##   0.522   0.1406495  0.8849572  0.09536613
##   0.523   0.1406783  0.8849350  0.09538930
##   0.524   0.1407072  0.8849127  0.09541252
##   0.525   0.1407360  0.8848905  0.09543577
##   0.526   0.1407649  0.8848682  0.09545903
##   0.527   0.1407938  0.8848459  0.09548228
##   0.528   0.1408226  0.8848237  0.09550552
##   0.529   0.1408515  0.8848015  0.09552882
##   0.530   0.1408802  0.8847794  0.09555209
##   0.531   0.1409090  0.8847573  0.09557537
##   0.532   0.1409377  0.8847353  0.09559856
##   0.533   0.1409663  0.8847134  0.09562170
##   0.534   0.1409947  0.8846917  0.09564466
##   0.535   0.1410230  0.8846702  0.09566761
##   0.536   0.1410511  0.8846490  0.09569044
##   0.537   0.1410792  0.8846277  0.09571327
##   0.538   0.1411072  0.8846066  0.09573603
##   0.539   0.1411353  0.8845856  0.09575879
##   0.540   0.1411632  0.8845646  0.09578152
##   0.541   0.1411911  0.8845437  0.09580424
##   0.542   0.1412191  0.8845228  0.09582697
##   0.543   0.1412471  0.8845018  0.09584970
##   0.544   0.1412751  0.8844808  0.09587250
##   0.545   0.1413031  0.8844598  0.09589532
##   0.546   0.1413312  0.8844388  0.09591817
##   0.547   0.1413594  0.8844176  0.09594107
##   0.548   0.1413875  0.8843965  0.09596402
##   0.549   0.1414158  0.8843753  0.09598701
##   0.550   0.1414440  0.8843541  0.09601001
##   0.551   0.1414723  0.8843328  0.09603305
##   0.552   0.1415006  0.8843115  0.09605611
##   0.553   0.1415290  0.8842901  0.09607919
##   0.554   0.1415574  0.8842687  0.09610228
##   0.555   0.1415858  0.8842472  0.09612540
##   0.556   0.1416143  0.8842257  0.09614853
##   0.557   0.1416428  0.8842042  0.09617166
##   0.558   0.1416714  0.8841826  0.09619483
##   0.559   0.1417000  0.8841610  0.09621803
##   0.560   0.1417286  0.8841393  0.09624123
##   0.561   0.1417573  0.8841176  0.09626445
##   0.562   0.1417860  0.8840958  0.09628770
##   0.563   0.1418148  0.8840740  0.09631105
##   0.564   0.1418436  0.8840522  0.09633442
##   0.565   0.1418724  0.8840303  0.09635785
##   0.566   0.1419012  0.8840084  0.09638139
##   0.567   0.1419301  0.8839864  0.09640500
##   0.568   0.1419591  0.8839644  0.09642867
##   0.569   0.1419881  0.8839423  0.09645235
##   0.570   0.1420171  0.8839202  0.09647604
##   0.571   0.1420461  0.8838981  0.09649974
##   0.572   0.1420752  0.8838759  0.09652338
##   0.573   0.1421041  0.8838539  0.09654694
##   0.574   0.1421331  0.8838319  0.09657049
##   0.575   0.1421621  0.8838098  0.09659406
##   0.576   0.1421912  0.8837877  0.09661767
##   0.577   0.1422202  0.8837656  0.09664129
##   0.578   0.1422493  0.8837434  0.09666491
##   0.579   0.1422783  0.8837214  0.09668840
##   0.580   0.1423073  0.8836994  0.09671187
##   0.581   0.1423363  0.8836774  0.09673536
##   0.582   0.1423652  0.8836554  0.09675882
##   0.583   0.1423941  0.8836335  0.09678229
##   0.584   0.1424229  0.8836117  0.09680570
##   0.585   0.1424516  0.8835900  0.09682906
##   0.586   0.1424802  0.8835685  0.09685224
##   0.587   0.1425086  0.8835472  0.09687539
##   0.588   0.1425369  0.8835261  0.09689840
##   0.589   0.1425652  0.8835050  0.09692138
##   0.590   0.1425934  0.8834840  0.09694430
##   0.591   0.1426216  0.8834631  0.09696721
##   0.592   0.1426497  0.8834423  0.09699010
##   0.593   0.1426778  0.8834215  0.09701294
##   0.594   0.1427059  0.8834008  0.09703577
##   0.595   0.1427340  0.8833800  0.09705859
##   0.596   0.1427621  0.8833593  0.09708146
##   0.597   0.1427903  0.8833385  0.09710433
##   0.598   0.1428185  0.8833177  0.09712720
##   0.599   0.1428467  0.8832968  0.09715008
##   0.600   0.1428750  0.8832759  0.09717305
##   0.601   0.1429032  0.8832550  0.09719606
##   0.602   0.1429316  0.8832340  0.09721909
##   0.603   0.1429599  0.8832130  0.09724214
##   0.604   0.1429883  0.8831919  0.09726522
##   0.605   0.1430167  0.8831708  0.09728832
##   0.606   0.1430452  0.8831497  0.09731143
##   0.607   0.1430737  0.8831285  0.09733457
##   0.608   0.1431022  0.8831073  0.09735773
##   0.609   0.1431308  0.8830860  0.09738091
##   0.610   0.1431594  0.8830648  0.09740409
##   0.611   0.1431880  0.8830434  0.09742729
##   0.612   0.1432167  0.8830221  0.09745049
##   0.613   0.1432454  0.8830007  0.09747373
##   0.614   0.1432741  0.8829793  0.09749702
##   0.615   0.1433028  0.8829578  0.09752032
##   0.616   0.1433316  0.8829363  0.09754365
##   0.617   0.1433605  0.8829147  0.09756700
##   0.618   0.1433893  0.8828931  0.09759039
##   0.619   0.1434182  0.8828715  0.09761380
##   0.620   0.1434471  0.8828498  0.09763724
##   0.621   0.1434761  0.8828281  0.09766070
##   0.622   0.1435051  0.8828064  0.09768417
##   0.623   0.1435341  0.8827846  0.09770764
##   0.624   0.1435632  0.8827628  0.09773111
##   0.625   0.1435923  0.8827410  0.09775463
##   0.626   0.1436214  0.8827191  0.09777816
##   0.627   0.1436506  0.8826971  0.09780170
##   0.628   0.1436796  0.8826753  0.09782516
##   0.629   0.1437087  0.8826535  0.09784857
##   0.630   0.1437378  0.8826317  0.09787202
##   0.631   0.1437669  0.8826098  0.09789555
##   0.632   0.1437960  0.8825880  0.09791910
##   0.633   0.1438251  0.8825661  0.09794262
##   0.634   0.1438543  0.8825442  0.09796617
##   0.635   0.1438834  0.8825224  0.09798960
##   0.636   0.1439124  0.8825006  0.09801302
##   0.637   0.1439415  0.8824788  0.09803646
##   0.638   0.1439705  0.8824571  0.09805984
##   0.639   0.1439995  0.8824354  0.09808322
##   0.640   0.1440285  0.8824137  0.09810660
##   0.641   0.1440573  0.8823922  0.09812988
##   0.642   0.1440861  0.8823708  0.09815307
##   0.643   0.1441147  0.8823496  0.09817612
##   0.644   0.1441432  0.8823285  0.09819909
##   0.645   0.1441716  0.8823076  0.09822193
##   0.646   0.1441999  0.8822868  0.09824472
##   0.647   0.1442282  0.8822660  0.09826748
##   0.648   0.1442565  0.8822454  0.09829018
##   0.649   0.1442847  0.8822247  0.09831294
##   0.650   0.1443129  0.8822042  0.09833565
##   0.651   0.1443410  0.8821837  0.09835833
##   0.652   0.1443692  0.8821632  0.09838104
##   0.653   0.1443973  0.8821427  0.09840372
##   0.654   0.1444255  0.8821222  0.09842640
##   0.655   0.1444537  0.8821017  0.09844910
##   0.656   0.1444819  0.8820812  0.09847180
##   0.657   0.1445102  0.8820606  0.09849449
##   0.658   0.1445385  0.8820400  0.09851724
##   0.659   0.1445668  0.8820193  0.09854000
##   0.660   0.1445951  0.8819986  0.09856276
##   0.661   0.1446235  0.8819779  0.09858555
##   0.662   0.1446519  0.8819571  0.09860836
##   0.663   0.1446803  0.8819364  0.09863121
##   0.664   0.1447088  0.8819155  0.09865408
##   0.665   0.1447373  0.8818947  0.09867700
##   0.666   0.1447658  0.8818738  0.09869997
##   0.667   0.1447944  0.8818529  0.09872294
##   0.668   0.1448229  0.8818319  0.09874593
##   0.669   0.1448515  0.8818110  0.09876893
##   0.670   0.1448802  0.8817900  0.09879193
##   0.671   0.1449088  0.8817689  0.09881493
##   0.672   0.1449375  0.8817478  0.09883794
##   0.673   0.1449663  0.8817267  0.09886096
##   0.674   0.1449950  0.8817056  0.09888400
##   0.675   0.1450238  0.8816844  0.09890707
##   0.676   0.1450526  0.8816632  0.09893014
##   0.677   0.1450814  0.8816419  0.09895326
##   0.678   0.1451103  0.8816206  0.09897638
##   0.679   0.1451392  0.8815993  0.09899955
##   0.680   0.1451681  0.8815780  0.09902273
##   0.681   0.1451971  0.8815566  0.09904593
##   0.682   0.1452261  0.8815352  0.09906914
##   0.683   0.1452551  0.8815137  0.09909236
##   0.684   0.1452841  0.8814922  0.09911558
##   0.685   0.1453132  0.8814707  0.09913881
##   0.686   0.1453423  0.8814492  0.09916207
##   0.687   0.1453714  0.8814276  0.09918535
##   0.688   0.1454005  0.8814060  0.09920864
##   0.689   0.1454296  0.8813844  0.09923189
##   0.690   0.1454587  0.8813630  0.09925510
##   0.691   0.1454877  0.8813415  0.09927830
##   0.692   0.1455168  0.8813199  0.09930151
##   0.693   0.1455459  0.8812984  0.09932475
##   0.694   0.1455751  0.8812768  0.09934797
##   0.695   0.1456042  0.8812553  0.09937118
##   0.696   0.1456333  0.8812337  0.09939441
##   0.697   0.1456624  0.8812122  0.09941751
##   0.698   0.1456914  0.8811908  0.09944058
##   0.699   0.1457205  0.8811693  0.09946366
##   0.700   0.1457495  0.8811479  0.09948671
##   0.701   0.1457784  0.8811266  0.09950970
##   0.702   0.1458074  0.8811052  0.09953270
##   0.703   0.1458363  0.8810840  0.09955565
##   0.704   0.1458650  0.8810628  0.09957851
##   0.705   0.1458937  0.8810418  0.09960127
##   0.706   0.1459223  0.8810210  0.09962395
##   0.707   0.1459507  0.8810003  0.09964658
##   0.708   0.1459790  0.8809798  0.09966910
##   0.709   0.1460074  0.8809593  0.09969159
##   0.710   0.1460356  0.8809388  0.09971405
##   0.711   0.1460638  0.8809186  0.09973643
##   0.712   0.1460920  0.8808982  0.09975882
##   0.713   0.1461202  0.8808780  0.09978120
##   0.714   0.1461483  0.8808578  0.09980351
##   0.715   0.1461764  0.8808377  0.09982583
##   0.716   0.1462045  0.8808175  0.09984816
##   0.717   0.1462326  0.8807974  0.09987045
##   0.718   0.1462608  0.8807772  0.09989278
##   0.719   0.1462889  0.8807571  0.09991512
##   0.720   0.1463171  0.8807368  0.09993746
##   0.721   0.1463454  0.8807166  0.09995981
##   0.722   0.1463736  0.8806963  0.09998217
##   0.723   0.1464019  0.8806760  0.10000456
##   0.724   0.1464302  0.8806557  0.10002698
##   0.725   0.1464585  0.8806354  0.10004941
##   0.726   0.1464868  0.8806150  0.10007186
##   0.727   0.1465152  0.8805946  0.10009437
##   0.728   0.1465436  0.8805741  0.10011690
##   0.729   0.1465720  0.8805537  0.10013944
##   0.730   0.1466004  0.8805332  0.10016199
##   0.731   0.1466289  0.8805126  0.10018454
##   0.732   0.1466574  0.8804921  0.10020709
##   0.733   0.1466859  0.8804715  0.10022967
##   0.734   0.1467145  0.8804509  0.10025224
##   0.735   0.1467430  0.8804302  0.10027483
##   0.736   0.1467716  0.8804096  0.10029746
##   0.737   0.1468002  0.8803889  0.10032013
##   0.738   0.1468289  0.8803681  0.10034279
##   0.739   0.1468575  0.8803474  0.10036546
##   0.740   0.1468862  0.8803266  0.10038815
##   0.741   0.1469149  0.8803058  0.10041087
##   0.742   0.1469437  0.8802849  0.10043360
##   0.743   0.1469725  0.8802640  0.10045636
##   0.744   0.1470012  0.8802431  0.10047913
##   0.745   0.1470301  0.8802222  0.10050191
##   0.746   0.1470589  0.8802012  0.10052468
##   0.747   0.1470878  0.8801802  0.10054748
##   0.748   0.1471167  0.8801592  0.10057027
##   0.749   0.1471456  0.8801382  0.10059307
##   0.750   0.1471745  0.8801171  0.10061587
##   0.751   0.1472035  0.8800960  0.10063868
##   0.752   0.1472325  0.8800748  0.10066149
##   0.753   0.1472615  0.8800537  0.10068430
##   0.754   0.1472905  0.8800325  0.10070711
##   0.755   0.1473196  0.8800112  0.10072992
##   0.756   0.1473486  0.8799901  0.10075268
##   0.757   0.1473775  0.8799690  0.10077539
##   0.758   0.1474065  0.8799479  0.10079809
##   0.759   0.1474355  0.8799268  0.10082079
##   0.760   0.1474645  0.8799056  0.10084350
##   0.761   0.1474935  0.8798844  0.10086620
##   0.762   0.1475225  0.8798633  0.10088886
##   0.763   0.1475516  0.8798421  0.10091151
##   0.764   0.1475806  0.8798209  0.10093415
##   0.765   0.1476095  0.8797999  0.10095667
##   0.766   0.1476385  0.8797788  0.10097917
##   0.767   0.1476674  0.8797578  0.10100167
##   0.768   0.1476963  0.8797368  0.10102415
##   0.769   0.1477252  0.8797158  0.10104656
##   0.770   0.1477540  0.8796948  0.10106897
##   0.771   0.1477828  0.8796739  0.10109131
##   0.772   0.1478116  0.8796532  0.10111359
##   0.773   0.1478402  0.8796325  0.10113581
##   0.774   0.1478687  0.8796120  0.10115792
##   0.775   0.1478971  0.8795916  0.10117997
##   0.776   0.1479254  0.8795713  0.10120195
##   0.777   0.1479536  0.8795512  0.10122387
##   0.778   0.1479818  0.8795311  0.10124579
##   0.779   0.1480100  0.8795111  0.10126772
##   0.780   0.1480381  0.8794912  0.10128956
##   0.781   0.1480662  0.8794713  0.10131139
##   0.782   0.1480942  0.8794514  0.10133323
##   0.783   0.1481222  0.8794316  0.10135508
##   0.784   0.1481502  0.8794119  0.10137692
##   0.785   0.1481782  0.8793921  0.10139876
##   0.786   0.1482062  0.8793724  0.10142057
##   0.787   0.1482342  0.8793527  0.10144236
##   0.788   0.1482622  0.8793329  0.10146414
##   0.789   0.1482903  0.8793132  0.10148593
##   0.790   0.1483184  0.8792934  0.10150776
##   0.791   0.1483464  0.8792736  0.10152961
##   0.792   0.1483745  0.8792537  0.10155147
##   0.793   0.1484027  0.8792339  0.10157335
##   0.794   0.1484308  0.8792140  0.10159523
##   0.795   0.1484590  0.8791941  0.10161712
##   0.796   0.1484872  0.8791741  0.10163903
##   0.797   0.1485154  0.8791541  0.10166095
##   0.798   0.1485436  0.8791342  0.10168287
##   0.799   0.1485719  0.8791141  0.10170482
##   0.800   0.1486002  0.8790941  0.10172678
##   0.801   0.1486285  0.8790740  0.10174875
##   0.802   0.1486568  0.8790539  0.10177074
##   0.803   0.1486851  0.8790338  0.10179276
##   0.804   0.1487135  0.8790137  0.10181478
##   0.805   0.1487419  0.8789935  0.10183683
##   0.806   0.1487703  0.8789733  0.10185890
##   0.807   0.1487987  0.8789531  0.10188099
##   0.808   0.1488272  0.8789329  0.10190310
##   0.809   0.1488556  0.8789126  0.10192523
##   0.810   0.1488841  0.8788923  0.10194737
##   0.811   0.1489126  0.8788720  0.10196951
##   0.812   0.1489412  0.8788517  0.10199166
##   0.813   0.1489697  0.8788313  0.10201381
##   0.814   0.1489983  0.8788109  0.10203596
##   0.815   0.1490269  0.8787905  0.10205814
##   0.816   0.1490555  0.8787700  0.10208033
##   0.817   0.1490842  0.8787496  0.10210252
##   0.818   0.1491128  0.8787291  0.10212472
##   0.819   0.1491415  0.8787085  0.10214692
##   0.820   0.1491702  0.8786880  0.10216913
##   0.821   0.1491990  0.8786674  0.10219135
##   0.822   0.1492277  0.8786468  0.10221361
##   0.823   0.1492565  0.8786262  0.10223588
##   0.824   0.1492853  0.8786055  0.10225814
##   0.825   0.1493141  0.8785849  0.10228046
##   0.826   0.1493429  0.8785642  0.10230281
##   0.827   0.1493718  0.8785434  0.10232517
##   0.828   0.1494007  0.8785227  0.10234755
##   0.829   0.1494295  0.8785019  0.10236993
##   0.830   0.1494584  0.8784812  0.10239227
##   0.831   0.1494871  0.8784606  0.10241461
##   0.832   0.1495159  0.8784400  0.10243695
##   0.833   0.1495447  0.8784193  0.10245928
##   0.834   0.1495736  0.8783986  0.10248162
##   0.835   0.1496024  0.8783779  0.10250396
##   0.836   0.1496313  0.8783572  0.10252626
##   0.837   0.1496601  0.8783365  0.10254854
##   0.838   0.1496890  0.8783157  0.10257082
##   0.839   0.1497178  0.8782951  0.10259302
##   0.840   0.1497465  0.8782746  0.10261514
##   0.841   0.1497752  0.8782540  0.10263728
##   0.842   0.1498040  0.8782334  0.10265944
##   0.843   0.1498327  0.8782129  0.10268157
##   0.844   0.1498613  0.8781924  0.10270369
##   0.845   0.1498900  0.8781719  0.10272582
##   0.846   0.1499186  0.8781515  0.10274791
##   0.847   0.1499472  0.8781312  0.10276993
##   0.848   0.1499756  0.8781110  0.10279185
##   0.849   0.1500040  0.8780908  0.10281371
##   0.850   0.1500322  0.8780709  0.10283547
##   0.851   0.1500604  0.8780510  0.10285719
##   0.852   0.1500885  0.8780312  0.10287883
##   0.853   0.1501165  0.8780117  0.10290037
##   0.854   0.1501445  0.8779921  0.10292197
##   0.855   0.1501724  0.8779725  0.10294359
##   0.856   0.1502003  0.8779531  0.10296516
##   0.857   0.1502282  0.8779337  0.10298675
##   0.858   0.1502560  0.8779143  0.10300834
##   0.859   0.1502839  0.8778950  0.10302988
##   0.860   0.1503117  0.8778757  0.10305136
##   0.861   0.1503395  0.8778565  0.10307285
##   0.862   0.1503673  0.8778372  0.10309434
##   0.863   0.1503950  0.8778180  0.10311579
##   0.864   0.1504228  0.8777987  0.10313727
##   0.865   0.1504507  0.8777795  0.10315877
##   0.866   0.1504785  0.8777602  0.10318028
##   0.867   0.1505063  0.8777409  0.10320182
##   0.868   0.1505342  0.8777216  0.10322338
##   0.869   0.1505621  0.8777022  0.10324494
##   0.870   0.1505900  0.8776829  0.10326651
##   0.871   0.1506179  0.8776635  0.10328811
##   0.872   0.1506459  0.8776441  0.10330974
##   0.873   0.1506739  0.8776246  0.10333138
##   0.874   0.1507018  0.8776052  0.10335303
##   0.875   0.1507298  0.8775857  0.10337468
##   0.876   0.1507579  0.8775662  0.10339637
##   0.877   0.1507859  0.8775467  0.10341810
##   0.878   0.1508140  0.8775271  0.10343986
##   0.879   0.1508420  0.8775076  0.10346165
##   0.880   0.1508701  0.8774880  0.10348346
##   0.881   0.1508982  0.8774684  0.10350529
##   0.882   0.1509264  0.8774487  0.10352714
##   0.883   0.1509545  0.8774291  0.10354900
##   0.884   0.1509827  0.8774094  0.10357090
##   0.885   0.1510109  0.8773897  0.10359282
##   0.886   0.1510391  0.8773700  0.10361474
##   0.887   0.1510673  0.8773503  0.10363667
##   0.888   0.1510955  0.8773305  0.10365860
##   0.889   0.1511238  0.8773107  0.10368053
##   0.890   0.1511521  0.8772909  0.10370248
##   0.891   0.1511804  0.8772711  0.10372443
##   0.892   0.1512087  0.8772512  0.10374639
##   0.893   0.1512370  0.8772314  0.10376836
##   0.894   0.1512654  0.8772115  0.10379033
##   0.895   0.1512938  0.8771915  0.10381230
##   0.896   0.1513221  0.8771716  0.10383430
##   0.897   0.1513505  0.8771516  0.10385630
##   0.898   0.1513790  0.8771317  0.10387830
##   0.899   0.1514074  0.8771117  0.10390031
##   0.900   0.1514359  0.8770916  0.10392232
##   0.901   0.1514644  0.8770716  0.10394435
##   0.902   0.1514929  0.8770515  0.10396642
##   0.903   0.1515214  0.8770314  0.10398849
##   0.904   0.1515499  0.8770113  0.10401057
##   0.905   0.1515785  0.8769911  0.10403265
##   0.906   0.1516070  0.8769710  0.10405473
##   0.907   0.1516356  0.8769508  0.10407682
##   0.908   0.1516642  0.8769306  0.10409890
##   0.909   0.1516929  0.8769104  0.10412099
##   0.910   0.1517215  0.8768901  0.10414307
##   0.911   0.1517500  0.8768700  0.10416510
##   0.912   0.1517786  0.8768499  0.10418709
##   0.913   0.1518071  0.8768298  0.10420910
##   0.914   0.1518356  0.8768096  0.10423111
##   0.915   0.1518642  0.8767895  0.10425314
##   0.916   0.1518928  0.8767693  0.10427517
##   0.917   0.1519214  0.8767491  0.10429720
##   0.918   0.1519499  0.8767289  0.10431919
##   0.919   0.1519785  0.8767088  0.10434121
##   0.920   0.1520071  0.8766886  0.10436323
##   0.921   0.1520356  0.8766685  0.10438513
##   0.922   0.1520640  0.8766485  0.10440705
##   0.923   0.1520925  0.8766285  0.10442897
##   0.924   0.1521210  0.8766084  0.10445089
##   0.925   0.1521494  0.8765884  0.10447278
##   0.926   0.1521778  0.8765685  0.10449461
##   0.927   0.1522062  0.8765486  0.10451642
##   0.928   0.1522346  0.8765286  0.10453824
##   0.929   0.1522629  0.8765088  0.10456000
##   0.930   0.1522911  0.8764891  0.10458171
##   0.931   0.1523192  0.8764694  0.10460334
##   0.932   0.1523473  0.8764499  0.10462488
##   0.933   0.1523752  0.8764305  0.10464632
##   0.934   0.1524031  0.8764112  0.10466770
##   0.935   0.1524309  0.8763920  0.10468902
##   0.936   0.1524586  0.8763730  0.10471024
##   0.937   0.1524863  0.8763540  0.10473146
##   0.938   0.1525140  0.8763349  0.10475269
##   0.939   0.1525416  0.8763160  0.10477386
##   0.940   0.1525692  0.8762972  0.10479500
##   0.941   0.1525968  0.8762783  0.10481615
##   0.942   0.1526244  0.8762595  0.10483729
##   0.943   0.1526519  0.8762407  0.10485841
##   0.944   0.1526794  0.8762220  0.10487948
##   0.945   0.1527069  0.8762033  0.10490056
##   0.946   0.1527344  0.8761845  0.10492166
##   0.947   0.1527619  0.8761659  0.10494270
##   0.948   0.1527894  0.8761472  0.10496375
##   0.949   0.1528169  0.8761285  0.10498480
##   0.950   0.1528445  0.8761098  0.10500586
##   0.951   0.1528720  0.8760910  0.10502692
##   0.952   0.1528996  0.8760723  0.10504797
##   0.953   0.1529272  0.8760535  0.10506905
##   0.954   0.1529548  0.8760347  0.10509013
##   0.955   0.1529824  0.8760159  0.10511123
##   0.956   0.1530100  0.8759970  0.10513232
##   0.957   0.1530377  0.8759782  0.10515341
##   0.958   0.1530653  0.8759593  0.10517450
##   0.959   0.1530930  0.8759404  0.10519560
##   0.960   0.1531207  0.8759215  0.10521669
##   0.961   0.1531484  0.8759026  0.10523778
##   0.962   0.1531761  0.8758836  0.10525888
##   0.963   0.1532039  0.8758647  0.10528001
##   0.964   0.1532316  0.8758457  0.10530118
##   0.965   0.1532594  0.8758267  0.10532237
##   0.966   0.1532872  0.8758077  0.10534359
##   0.967   0.1533150  0.8757886  0.10536483
##   0.968   0.1533428  0.8757695  0.10538609
##   0.969   0.1533706  0.8757505  0.10540735
##   0.970   0.1533985  0.8757314  0.10542861
##   0.971   0.1534264  0.8757122  0.10544989
##   0.972   0.1534542  0.8756931  0.10547119
##   0.973   0.1534821  0.8756739  0.10549251
##   0.974   0.1535101  0.8756548  0.10551385
##   0.975   0.1535380  0.8756356  0.10553519
##   0.976   0.1535659  0.8756163  0.10555654
##   0.977   0.1535939  0.8755971  0.10557793
##   0.978   0.1536219  0.8755779  0.10559931
##   0.979   0.1536499  0.8755586  0.10562070
##   0.980   0.1536779  0.8755393  0.10564211
##   0.981   0.1537059  0.8755200  0.10566351
##   0.982   0.1537340  0.8755006  0.10568492
##   0.983   0.1537620  0.8754813  0.10570633
##   0.984   0.1537901  0.8754619  0.10572773
##   0.985   0.1538182  0.8754425  0.10574918
##   0.986   0.1538463  0.8754231  0.10577066
##   0.987   0.1538744  0.8754037  0.10579215
##   0.988   0.1539025  0.8753842  0.10581367
##   0.989   0.1539307  0.8753648  0.10583521
##   0.990   0.1539588  0.8753453  0.10585677
##   0.991   0.1539870  0.8753258  0.10587836
##   0.992   0.1540152  0.8753062  0.10589995
##   0.993   0.1540434  0.8752867  0.10592154
##   0.994   0.1540717  0.8752671  0.10594314
##   0.995   0.1540999  0.8752475  0.10596475
##   0.996   0.1541282  0.8752279  0.10598637
##   0.997   0.1541565  0.8752083  0.10600800
##   0.998   0.1541847  0.8751886  0.10602966
##   0.999   0.1542130  0.8751690  0.10605130
##   1.000   0.1542412  0.8751495  0.10607291
## 
## Tuning parameter 'alpha' was held constant at a value of 0
## RMSE was used to select the optimal model using  the smallest value.
## The final values used for the model were alpha = 0 and lambda = 0.089.
```


```r
ggplot(data=dplyr::filter(model_ridge$result,RMSE<0.14)) +  #yk
    geom_line(aes(x=lambda,y=RMSE))
```

![](/assets/img/unnamed-chunk-1-1.png)<!-- -->


```r
mean(model_ridge$resample$RMSE)
```

```
## [1] 0.1308965
```

----------  

#### Lasso

```r
# test out Lasso regression model

# train model
set.seed(123)  # for reproducibility
model_lasso <- train(x=X_train,y=y,
                  method="glmnet",
                  metric="RMSE",
                  maximize=FALSE,
                  trControl=CARET.TRAIN.CTRL,
                  tuneGrid=expand.grid(alpha=1,  # Lasso regression
                                       lambda=c(1,0.1,0.05,0.01,seq(0.009,0.001,-0.001),
                                            0.00075,0.0005,0.0001)))
model_lasso
```

```
## glmnet 
## 
## 1460 samples
##  288 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 5 times) 
## Summary of sample sizes: 1169, 1168, 1167, 1167, 1169, 1168, ... 
## Resampling results across tuning parameters:
## 
##   lambda   RMSE       Rsquared   MAE       
##   0.00010  0.1364073  0.8849857  0.08588795
##   0.00050  0.1315241  0.8921940  0.08329823
##   0.00075  0.1304120  0.8938013  0.08239260
##   0.00100  0.1296366  0.8949063  0.08170930
##   0.00200  0.1274528  0.8980568  0.07990676
##   0.00300  0.1263226  0.8997518  0.07930004
##   0.00400  0.1260769  0.9001275  0.07953154
##   0.00500  0.1262120  0.8999959  0.08014017
##   0.00600  0.1265722  0.8995779  0.08090337
##   0.00700  0.1270563  0.8990182  0.08172069
##   0.00800  0.1277540  0.8981586  0.08262834
##   0.00900  0.1286558  0.8970003  0.08362046
##   0.01000  0.1296635  0.8956888  0.08465664
##   0.05000  0.1729848  0.8397762  0.12102571
##   0.10000  0.2156593  0.7948456  0.15744685
##   1.00000  0.3991753        NaN  0.30981341
## 
## Tuning parameter 'alpha' was held constant at a value of 1
## RMSE was used to select the optimal model using  the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.004.
```

```r
ggplot(data=dplyr::filter(model_lasso$result,RMSE<0.14)) +  #yk
    geom_line(aes(x=lambda,y=RMSE))
```

![](/assets/img/unnamed-chunk-2-1.png)<!-- -->


```r
mean(model_lasso$resample$RMSE)
```

```
## [1] 0.1260769
```

----------  


#### Elastic Net  


```r
# test out Lasso regression model

# train model
set.seed(123)  # for reproducibility
model_enet <- train(x=X_train,y=y,
                  method="glmnet",
                  metric="RMSE",
                  maximize=FALSE,
                  trControl=CARET.TRAIN.CTRL,
                  tuneGrid=expand.grid(alpha=0.5,  # elastic net regression
                                       lambda=c(1,0.1,0.05,0.01,seq(0.009,0.001,-0.001),
                                            0.00075,0.0005,0.0001)))
model_enet
```

```
## glmnet 
## 
## 1460 samples
##  288 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold, repeated 5 times) 
## Summary of sample sizes: 1169, 1168, 1167, 1167, 1169, 1168, ... 
## Resampling results across tuning parameters:
## 
##   lambda   RMSE       Rsquared   MAE       
##   0.00010  0.1370848  0.8839591  0.08624414
##   0.00050  0.1335460  0.8891837  0.08470957
##   0.00075  0.1323424  0.8909650  0.08394022
##   0.00100  0.1314368  0.8922986  0.08325852
##   0.00200  0.1295427  0.8950203  0.08166458
##   0.00300  0.1282706  0.8968560  0.08063284
##   0.00400  0.1272969  0.8982799  0.07989033
##   0.00500  0.1265961  0.8993366  0.07948422
##   0.00600  0.1261347  0.9000432  0.07931230
##   0.00700  0.1259230  0.9003770  0.07934258
##   0.00800  0.1258752  0.9004792  0.07952737
##   0.00900  0.1259191  0.9004637  0.07980596
##   0.01000  0.1260284  0.9003659  0.08015747
##   0.05000  0.1487409  0.8731837  0.10040044
##   0.10000  0.1753950  0.8467994  0.12211971
##   1.00000  0.3991753        NaN  0.30981341
## 
## Tuning parameter 'alpha' was held constant at a value of 0.5
## RMSE was used to select the optimal model using  the smallest value.
## The final values used for the model were alpha = 0.5 and lambda = 0.008.
```

```r
ggplot(data=dplyr::filter(model_enet$result,RMSE<0.14)) +  #yk
    geom_line(aes(x=lambda,y=RMSE))
```

![](/assets/img/unnamed-chunk-3-1.png)<!-- -->

```r
mean(model_enet$resample$RMSE)
```

```
## [1] 0.1258752
```


```r
# extract coefficients for the best performing model
coef <- data.frame(coef.name = dimnames(coef(model_lasso$finalModel,s=model_lasso$bestTune$lambda))[[1]], 
           coef.value = matrix(coef(model_lasso$finalModel,s=model_lasso$bestTune$lambda)))

# exclude the (Intercept) term
coef <- coef[-1,]
```

----------  

### Feature Selection  

coef.value가 0인 항목은 feature selection에 포함되지 않는 항목  
coef.value가 0이 아닌 항목은 feature selection 항목  
coef.name이 해당 계수의 변수 이름 추출하게 됨  

```r
# extract coefficients for the best performing model
coef_enet <- data.frame(coef_enet.name = dimnames(coef(model_enet$finalModel,s=model_enet$bestTune$lambda))[[1]], 
           coef_enet.value = matrix(coef(model_enet$finalModel,s=model_enet$bestTune$lambda)))

# exclude the (Intercept) term
coef_enet <- coef_enet[-1,]
```


```r
# print summary of model results
picked_features_enet <- nrow(dplyr::filter(coef_enet,coef_enet.value!=0)) #yk dplyr::filter
enet_features <- dplyr::filter(coef_enet,coef_enet.value!=0)
not_picked_features_enet <- nrow(dplyr::filter(coef_enet,coef_enet.value==0))

cat("Elastic Net picked",picked_features_enet,"variables and eliminated the other",
    not_picked_features_enet,"variables\n")
```

```
## Elastic Net picked 110 variables and eliminated the other 178 variables
```

추출된 변수 확인  

```r
# sort coefficients in ascending order
coef_enet <- dplyr::arrange(coef_enet,-coef_enet.value)

# extract the top 10 and bottom 10 features
imp_coef_enet <- rbind(head(coef_enet,10),
                  tail(coef_enet,10))
```



```r
ggplot(imp_coef_enet) +
    geom_bar(aes(x=reorder(coef_enet.name,coef_enet.value),y=coef_enet.value),
             stat="identity") +
    ylim(-1.5,0.6) +
    coord_flip() +
    ggtitle("Coefficents in the Elastic Net Model") +
    theme(axis.title=element_blank())
```

![](/assets/img/In_23-1.png)<!-- -->


```r
# print summary of model results
picked_features <- nrow(dplyr::filter(coef,coef.value!=0)) #yk dplyr::filter
lasso_features <- dplyr::filter(coef,coef.value!=0)
not_picked_features <- nrow(dplyr::filter(coef,coef.value==0))

cat("Lasso picked",picked_features,"variables and eliminated the other",
    not_picked_features,"variables\n")
```


```
## Lasso picked 108 variables and eliminated the other 180 variables
```


```r
# sort coefficients in ascending order
coef <- dplyr::arrange(coef,-coef.value)

# extract the top 10 and bottom 10 features
imp_coef <- rbind(head(coef,10),
                  tail(coef,10))
```

양/음으로 상관계수 큰 변수 확인 (10개씩)  
```r
ggplot(imp_coef) +
    geom_bar(aes(x=reorder(coef.name,coef.value),y=coef.value),
             stat="identity") +
    ylim(-1.5,0.6) +
    coord_flip() +
    ggtitle("Coefficents in the Lasso Model") +
    theme(axis.title=element_blank())
```

![](/assets/img/In_26-1.png)<!-- -->

lasso의 모든 feature는 enet에 포함되어 있음  
```r
# make create submission file
common_feature <- intersect(lasso_features$coef.name, enet_features$coef_enet.name)
diff_feature <- setdiff(enet_features$coef_enet.name, lasso_features$coef.name)
```  

----------  

### Summary  
- number of Features 
Ridge는 변수 제거는 하지 않고 계수를 축소하는 효과를 가진다    
Lasso는 108개, Enet은 110개로 feature selection   
Ridge : 288, Lasso : 108, Enet : 110  
Lasso의 108개 feature들은 Enet의 110개 feature에 모두 포함되어 있다  
- RMSE  
Enet의 RMSE가 최소값을 가진다  
Ridge : 0.1309, Lasso : 0.1260, Enet : 0.1258  
- Parameter   
Caret Package를 통한 lambda tuning parameter는 다음과 같다  
Ridge : alpha=0, lambda=0.089  
Lasso : alpha=1, lambda=0.004  
Enet : alpha=0.5, lambda=0.008  


----------  

예측 후 답 제출  
```r
# make create submission file
preds <- exp(predict(model_lasso,newdata=X_test)) - 1
```


```r
# construct data frame for solution
solution <- data.frame(Id=as.integer(rownames(X_test)),SalePrice=preds)
write.csv(solution,"ridge_sol.csv",row.names=FALSE)
```

----------

### Reference
1) ISLR chap6. Variable Selection  
2) [J. Thompson](https://www.kaggle.com/jimthompson)'s 
[Jupyter-Notebook Kernel](https://www.kaggle.com/jimthompson/regularized-linear-models-in-r/notebook).


