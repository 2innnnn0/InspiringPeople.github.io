<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.12.0 by Michael Rose
  Copyright 2013-2018 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Word2Vec &amp; FastText (이론) - Inspiring People</title>
<meta name="description" content="0, 1만 알아들을 수 있는 컴퓨터에게 우리의 언어를 이해시키기 위해서는 어떠한 작업들이 필요할까?">



<meta property="og:type" content="article">
<meta property="og:locale" content="ko">
<meta property="og:site_name" content="Inspiring People">
<meta property="og:title" content="Word2Vec &amp; FastText (이론)">
<meta property="og:url" content="http://localhost:4000/data%20analysis/word_embedding/">


  <meta property="og:description" content="0, 1만 알아들을 수 있는 컴퓨터에게 우리의 언어를 이해시키기 위해서는 어떠한 작업들이 필요할까?">



  <meta property="og:image" content="http://localhost:4000/assets/img/teaser.png">





  <meta property="article:published_time" content="2018-08-29T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/data%20analysis/word_embedding/">





  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Organization",
      "url": "http://localhost:4000",
      "logo": "http://localhost:4000/assets/img/teaser.png"
    }
  </script>



  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "YounKyung Jang",
      "url": "http://localhost:4000",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Inspiring People Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="/">Inspiring People</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/categories/" >Category</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/tags/" >Tag</a>
            </li>
          
            
            <li class="masthead__menu-item">
              <a href="http://localhost:4000/archive/" >Archive</a>
            </li>
          
        </ul>
        
        <button class="search__toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/img/profile.jpg" alt="YounKyung Jang" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">YounKyung Jang</h3>
    
    
      <p class="author__bio" itemprop="description">
        자세히 보아야 아름답다. 너도 그렇다
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      

      

      
        <li>
          <a href="mailto:ykjang@gmail.com">
            <meta itemprop="email" content="ykjang@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> 이메일
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      
        <li>
          <a href="https://github.com/InspiringPeople" itemprop="sameAs">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      
        
          <li>
            <a href="https://youtube.com/dearpiano" itemprop="sameAs">
              <i class="fab fa-fw fa-youtube" aria-hidden="true"></i> YouTube
            </a>
          </li>
        
      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>  

  
    <div class="author__avatar2">
      

      
        <img src="/assets/img/profile2.jpg" alt="YounKyung Jang" itemprop="image">
      
    </div>
  
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Word2Vec &amp; FastText (이론)">
    <meta itemprop="description" content="0, 1만 알아들을 수 있는 컴퓨터에게 우리의 언어를 이해시키기 위해서는 어떠한 작업들이 필요할까?">
    <meta itemprop="datePublished" content="August 29, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Word2Vec &amp; FastText (이론)
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On This Page</h4></header>
              <ul class="toc__menu">
  <li><a href="#fasttext-word2vec">FastText (+Word2Vec)</a>
    <ul>
      <li><a href="#word-embedding-이란">Word Embedding 이란?</a></li>
    </ul>
  </li>
  <li><a href="#1-word2vec">1. Word2Vec</a></li>
  <li><a href="#word2vec으로-할-수-있는-것들">Word2Vec으로 할 수 있는 것들</a>
    <ul>
      <li><a href="#벡터-연산을-통한-추론">벡터 연산을 통한 추론</a></li>
      <li><a href="#clustering">Clustering</a></li>
      <li><a href="#classification">Classification</a></li>
      <li><a href="#word2vec의-한계점">Word2Vec의 한계점</a></li>
    </ul>
  </li>
  <li><a href="#2-fasttext">2. FastText</a>
    <ul>
      <li><a href="#1-pre-trained-fasttext">1. Pre-trained FastText</a></li>
    </ul>
  </li>
  <li><a href="#visualization">Visualization</a>
    <ul>
      <li><a href="#2-self-trained-fasttext-by-fasttext-api">2. Self-trained FastText by FastText API</a></li>
      <li><a href="#3-self-trained-fasttext-by-gensim">3. Self-trained FastText by Gensim</a></li>
      <li><a href="#내가-가진-데이터로-fasttext-모델을-만들어보기">내가 가진 데이터로 FastText 모델을 만들어보기</a></li>
      <li><a href="#word2vec-vs-fasttext">Word2Vec VS FastText</a></li>
    </ul>
  </li>
  <li><a href="#rerefence">Rerefence</a></li>
</ul>
            </nav>
          </aside>
        
        <p>0, 1만 알아들을 수 있는 컴퓨터에게 우리의 언어를 이해시키기 위해서는 어떠한 작업들이 필요할까?</p>

<p>그 해답은 바로 Word Embedding에 있다.<br />
Word Embedding 여러 기법 중 대표적인 Word2Vec과 FastText를 설명한다.</p>

<h2 id="fasttext-word2vec">FastText (+Word2Vec)</h2>

<p class="text-center"><img src="/assets/img/title.png" height="400" width="600" align="center" class="align-center" /><br />
(그림작업:애플펜슬, featuring: NEO in KaKaoFriends)</p>

<hr />
<h1 id="nlp-steps--활용">NLP Steps &amp; 활용</h1>

<p><img src="/assets/img/nlp.png" height="400" width="600" class="align-center" /></p>

<h1 id="word-embedding">Word Embedding</h1>

<h3 id="word-embedding-이란">Word Embedding 이란?</h3>

<p>비정형화된 Text를 숫자로 바꿔줌으로써 사람의 언어를 컴퓨터의 언어로 번역하는 것</p>

<p><strong>국소표현 (local representation)</strong></p>
<ul>
  <li>one-hot encoding : Text를 숫자로 바꿔주지만 단어간 유사도를 측정하기 어렵다</li>
  <li>TF-IDF : 단어가 문서에 (n번) 존재함을 기준으로 중요도를 구하고 단어 중요도의 가중합을 구함 (순서를 고려하지 않는 bag of words 방식)</li>
  <li>sparse, long vectors</li>
</ul>

<p><strong>분산표현 (distributed representation)</strong></p>
<ul>
  <li>word2vec (google)</li>
  <li>fasttext (facebook)</li>
  <li>glove (stanford)</li>
  <li>dense, short vectors</li>
</ul>

<p><img src="/assets/img/local_distributed.png" alt="local_distributed.png" /></p>

<h2 id="1-word2vec">1. Word2Vec</h2>

<p>(Tomas Mikolov. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.)</p>

<p><strong>Distributional Hypothesis</strong></p>
<ul>
  <li>두 단어의 문맥(주변 단어들)/분포도가 비슷하면 “의미적”으로 유사한 단어</li>
  <li>예) 집 앞 편의점에서 아이스크림을 사 먹었는데, ___ 시려서 너무 먹기가 힘들었다.</li>
</ul>

<p><strong>Word2Vec</strong></p>
<ul>
  <li>Distributional Hyopthesis 기반으로 단어의 의미와 맥락을 고려하여 단어를 벡터로 표현하는 방법</li>
  <li>주변단어(window) Size에 따라 말뭉치를 슬라이딩하면서 중심단어별의 주변단어들을 보고 각 단어의 벡터값을 업데이트 해나가는 방식으로 훈련</li>
  <li>window 내에 등장하지 않는 단어에 해당하는 벡터는 중심단어 벡터와 벡터공간상에서 멀어지게끔(내적값 줄이기), 등장하는 주변단어 벡터는 중심단어 벡터와 가까워지게끔(내적값 키우기) 값을 변경해 나감</li>
</ul>

<p><img src="/assets/img/cbow_skipgram.PNG" alt="cbow_skipgram.PNG" /></p>

<p><strong>CBoW &amp; Skip-gram 예시</strong></p>

<p>CBoW : 나는 향긋한 —를 좋아한다.<br />
Skip-gram : — 외나무다리에서 —.</p>

<h2 id="word2vec으로-할-수-있는-것들">Word2Vec으로 할 수 있는 것들</h2>
<h3 id="벡터-연산을-통한-추론">벡터 연산을 통한 추론</h3>
<p>vector(‘Paris’) - vector(‘France’) + vector(‘Italy’) = vector(‘Rome’)
<img src="/assets/img/word2vec.png" alt="word2vec.png" /></p>

<h3 id="clustering">Clustering</h3>
<p><img src="/assets/img/wv_clustering.PNG" alt="wv_clustering.PNG" /></p>

<h3 id="classification">Classification</h3>
<p><img src="/assets/img/wv_classification.PNG" height="200" width="400" /></p>

<h3 id="word2vec의-한계점">Word2Vec의 한계점</h3>
<ul>
  <li><strong>단어의 형태학적 특성을 반영하지 못함</strong><br />
예를들어, teach와 teacher, teachers 세 단어는 의미적으로 유사한 단어임이 분명하다. 그런데 과거의 Word2Vec이나 Glove등과 같은 방법들은 이러한 단어들을 개별적으로 Embedding하기 때문에 셋의 Vector가 유사하게 구성되지 않는다.<br />
<br /></li>
  <li><strong>희소한 단어를 Embedding하기 어려움</strong><br />
Word2Vec등과 같은 기존의 방법들은 Distribution hypothesis를 기반으로 학습하는 것이기 때문에, 출현횟수가 많은 단어에 대해서는 잘 Embedding이 되지만, 출현횟수가 적은 단어에 대해서는 제대로 Embedding이 되지 않는다.<br />
(Machine learning에서, Sample이 적은 단어에 대해서는 Underfitting이 되는 것처럼)<br />
<br /></li>
  <li><strong>Out-of-Vocabulary(OOV)를 처리할 수 없는 단점</strong>  <br />
Word2Vec은 단어단위로 어휘집(Vocabulary)를 구성하기 때문에, 어휘집에 없는 새로운 단어가 등장하면 데이터 전체를 다시 학습시켜야 함</li>
</ul>

<h2 id="2-fasttext">2. FastText</h2>
<p><strong>논문</strong></p>
<ol>
  <li>Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov. Enriching Word Vectors with Subword Information, 2016</li>
  <li>Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, Armand Joulin. Advances in Pre-Training Distributed Word Representations, 2017</li>
</ol>

<p><strong>Facebook에서 발표한 Word Embedding 기법으로 Word2vec과 비교하여 다음과 같은 차별점이 있음</strong></p>

<ul>
  <li>Word embedding (Distributed vector represenatation of words)에는 다양한 방법이 있지만, 대부분의 방법들은 언어의 형태학적(Morpological)인 특성을 반영하지 못하고, 또 희소한 단어에 대해서는 Embedding이 되지 않음</li>
  <li>본 연구에서는 단어를 Bag-of-Characters로 보고, 개별 단어가 아닌 n-gram의 Charaters를 Embedding함 (Skip-gram model 사용)</li>
  <li>최종적으로 각 단어는 Embedding된 n-gram의 합으로 표현됨, 그 결과 빠르고 좋은 성능을 나타냈음</li>
</ul>

<h1 id="fasttext-example">FastText Example</h1>

<h3 id="1-pre-trained-fasttext">1. Pre-trained FastText</h3>
<p>아래 링크에서 wikipedia 데이터 기반으로 학습된 전세계 294개 언어로 된 pre-trained fastText를 제공하고 있음<br />
(parameter : 300 dimension, skip-gram model)<br />
한국어의 경우 wiki.ko.bin / wiki.ko.vec 파일<br />
https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md</p>

<p>Pre-trained 된 FastText는 fastText API 또는 Gensim 을 이용하여 로드해서 활용</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c"># Creating the model</span>
<span class="n">ko_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s">'wiki.ko.vec'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Getting the tokens </span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">ko_model</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
    <span class="n">words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

<span class="c"># Printing out number of tokens available</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Number of Tokens: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)))</span>

<span class="c"># Printing out the dimension of a word vector </span>
<span class="k">print</span><span class="p">(</span><span class="s">"Dimension of a word vector: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="nb">len</span><span class="p">(</span><span class="n">ko_model</span><span class="p">[</span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="p">))</span>

<span class="c"># Print out the vector of a word </span>
<span class="k">print</span><span class="p">(</span><span class="s">"Vector components of a word: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">ko_model</span><span class="p">[</span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Number of Tokens: 879129
Dimension of a word vector: 300
Vector components of a word: [ 4.0987e-01  5.3006e-03 -1.5832e+00 -1.0234e+00  2.7239e-01 -1.2325e+00
 -2.8500e-01 -5.9057e-01 -7.1622e-01 -7.8779e-01  4.6649e-02  3.1382e-01
 -4.1487e-01 -9.1984e-01 -3.0980e-01 -1.5516e-01  8.1917e-02 -8.8866e-01
 -2.6710e-02 -7.6736e-01  1.0054e+00 -2.7689e-01  6.0095e-01 -6.6004e-02
  5.7456e-01  6.7092e-01 -1.4202e-01  3.1292e-01 -8.1834e-01  3.1503e-01
  8.6697e-01 -9.3468e-01 -1.0193e+00  3.2536e-01 -6.4223e-01 -7.6901e-01
 -1.3965e+00 -1.2300e+00 -2.8656e-01 -3.4853e-01  1.0772e+00  1.2494e+00
  3.3720e-01 -7.6690e-01  6.3737e-01 -4.4553e-01  5.1555e-01 -3.8258e-01
  8.6264e-01 -6.9718e-01  1.4699e+00  6.7000e-01 -1.2923e+00 -1.0476e-01
  9.5305e-01  4.7174e-02  1.0691e+00  6.5087e-01  1.4713e+00 -8.3216e-01
  7.1885e-01  1.8395e+00 -4.4246e-01  3.1631e-01  7.3043e-02 -1.9448e+00
  1.0989e+00 -2.6499e+00  1.2871e+00 -1.2371e-01 -6.1374e-01  2.7363e-01
 -4.9095e-01  6.3166e-01 -5.0715e-01  1.5388e+00  1.2357e+00  1.1372e-01
 -1.5515e-01 -1.2813e-01 -8.8398e-01  2.7790e-01  2.6742e+00 -6.7633e-01
 -3.9882e-01  1.4353e+00  6.2972e-01 -8.2433e-01  1.0755e-01  2.6077e-01
 -2.6062e-02  2.1041e+00  1.6706e+00 -9.5291e-01 -5.6517e-01  1.1869e+00
  2.9987e+00  7.9352e-01 -1.9160e+00  1.3003e+00  1.1930e+00 -4.1752e-01
 -2.5951e-01 -3.6834e-01  4.0138e-01  5.0667e-01 -7.5915e-01 -2.0810e+00
 -1.3750e+00 -1.7526e-01 -2.6753e-01  2.2860e+00  5.3970e-01  6.9420e-01
 -2.9786e-01  4.0885e-01  1.2528e+00 -1.3112e+00 -8.6995e-01 -1.3691e+00
 -1.0839e+00  1.6241e+00  3.4093e-01 -1.5496e+00 -5.3989e-01  5.7118e-01
 -2.3308e+00 -1.2780e+00  3.3024e-01 -3.5477e+00 -3.0986e-01 -1.0491e+00
  1.6221e+00 -7.3885e-01  4.9361e-01  1.1624e+00 -5.4816e-01 -1.4676e-01
 -7.2129e-01  6.1484e-02 -6.6246e-01 -1.7471e+00 -3.5120e-02 -1.5069e+00
  1.5748e+00 -1.9118e+00  5.9579e-01  1.7077e+00 -8.6890e-01  9.6898e-01
  6.1799e-01  9.0997e-01  1.0470e+00 -1.2207e+00  1.0459e+00  5.8882e-01
 -2.6000e+00 -1.1357e+00 -6.0270e-01  8.1014e-01  2.1929e+00 -3.0279e+00
  1.4206e+00 -1.2978e+00  1.8195e-01 -2.6331e+00 -1.1074e-01  3.8271e-02
  8.5635e-02 -1.7056e+00 -9.7833e-01  1.8130e+00  1.5086e+00 -3.5239e+00
 -1.5104e+00 -4.3756e-01 -1.4024e+00  3.8206e-01  2.5983e+00  1.4196e+00
 -5.1332e-01  2.1280e+00 -1.5393e+00 -3.7143e-01  3.3290e+00 -3.0475e+00
  1.1043e+00 -1.4929e+00  1.1566e+00 -1.2335e+00  2.1575e+00  1.4761e+00
  7.7081e-01  3.0180e+00 -1.7762e+00  1.0372e+00 -2.5132e+00 -1.3457e+00
  5.7445e+00 -2.2810e+00  1.7091e+00 -2.6253e+00 -1.8589e+00 -1.0106e+00
 -2.6499e+00  1.5363e-01 -3.1414e-01 -5.5227e-01 -1.0618e+00  3.1632e-01
  1.6115e+00  1.5533e+00  6.0743e-01  1.2819e-01 -3.1507e-01 -2.9873e+00
 -2.5450e+00  3.4801e-02 -1.2129e+00  1.9365e-01  2.8118e+00  1.8606e+00
  2.0968e+00 -1.1329e+00  2.7094e+00  6.1029e-01  1.1825e+00 -1.5155e+00
 -2.3947e+00  7.7830e-01 -3.5778e-01  1.7861e+00  8.1221e-01 -9.2864e-01
 -6.4006e-01  8.1047e-01 -1.3697e+00  4.1603e+00  1.2102e+00  1.8280e+00
  5.4124e-01 -1.1996e+00 -6.2871e-01  1.5486e-02 -1.3865e+00 -7.6308e-01
 -3.3788e-01  9.4906e-01  1.4133e+00 -6.9891e-01  1.0454e+00  7.3367e-01
  4.4642e-01 -1.2267e+00  3.9124e-01  2.3565e+00 -2.2171e+00 -1.9194e+00
  1.7481e+00  4.7839e-01 -3.6169e-01 -2.2142e+00  2.6424e+00 -2.3072e+00
 -1.4583e+00 -4.7219e-03  5.5658e-01  1.7730e+00  1.0415e+00  1.4673e+00
  8.4926e-01 -2.1306e-01  2.3512e-01 -2.1706e+00 -5.5120e-01  4.0255e-01
  4.2331e-01 -1.5328e+00  7.0063e-01  8.0445e-01  1.3166e+00  2.7564e+00
 -1.9342e+00  1.5141e+00 -6.9278e-01 -1.5425e+00 -2.1596e+00 -2.3332e+00
 -2.1304e+00  1.2098e+00 -2.3404e+00  8.7899e-01  1.5998e+00  1.6685e+00
  1.7393e+00 -4.2142e-01 -7.9255e-01 -3.5900e+00  6.8225e-01 -3.8677e+00]
&lt;/s&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">words</span><span class="p">[</span><span class="mi">50</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'가'
</code></pre></div></div>

<p>pre-trained 된 fastText 모델로 단어간 유사도 검색을 할 수 있다</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c"># Pick a word </span>
<span class="n">find_similar_to</span> <span class="o">=</span> <span class="s">'사랑'</span>

<span class="c"># Finding out similar words [default= top 10]</span>
<span class="k">for</span> <span class="n">similar_word</span> <span class="ow">in</span> <span class="n">ko_model</span><span class="o">.</span><span class="n">similar_by_word</span><span class="p">(</span><span class="n">find_similar_to</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Word: {0}, Similarity: {1:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">similar_word</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">similar_word</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">))</span>


</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Word: 사랑사랑, Similarity: 0.81
Word: 사랑치, Similarity: 0.78
Word: 사랑일, Similarity: 0.77
Word: 사랑느낌, Similarity: 0.76
Word: 사랑이었네, Similarity: 0.76
Word: 사랑이여, Similarity: 0.75
Word: 사랑병, Similarity: 0.75
Word: 사랑인, Similarity: 0.75
Word: 사랑맛, Similarity: 0.75
Word: 사랑노래, Similarity: 0.74
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Test words </span>
<span class="n">word_add</span> <span class="o">=</span> <span class="p">[</span><span class="s">'동물'</span><span class="p">,</span> <span class="s">'파충류'</span><span class="p">]</span>
<span class="n">word_sub</span> <span class="o">=</span> <span class="p">[</span><span class="s">'뱀'</span><span class="p">]</span>

<span class="c"># Word vector addition and subtraction </span>
<span class="k">for</span> <span class="n">resultant_word</span> <span class="ow">in</span> <span class="n">ko_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span>
    <span class="n">positive</span><span class="o">=</span><span class="n">word_add</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="n">word_sub</span>
<span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Word : {0} , Similarity: {1:.2f}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">resultant_word</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">resultant_word</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">))</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Word : 포유류 , Similarity: 0.72
Word : 포유동물 , Similarity: 0.71
Word : 절지동물 , Similarity: 0.69
Word : 양서류 , Similarity: 0.69
Word : 독동물 , Similarity: 0.69
Word : 포유류분류 , Similarity: 0.68
Word : 무척추동물 , Similarity: 0.68
Word : 척추동물분류 , Similarity: 0.68
Word : 도시동물 , Similarity: 0.68
Word : 동물상 , Similarity: 0.67
</code></pre></div></div>

<h2 id="visualization"><strong>Visualization</strong></h2>
<p>wi.ko.vec에 저장된 88만여개 단어 중 선두 300개 단어를 2차원 상에 맵핑하면 다음과 같은 그림이 보여짐</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">font_manager</span><span class="p">,</span> <span class="n">rc</span><span class="p">,</span> <span class="n">rcParams</span>
<span class="n">font_name</span> <span class="o">=</span> <span class="n">font_manager</span><span class="o">.</span><span class="n">FontProperties</span><span class="p">(</span><span class="n">fname</span><span class="o">=</span><span class="s">"/usr/share/fonts/truetype/MALGUN.TTF"</span><span class="p">)</span><span class="o">.</span><span class="n">get_name</span><span class="p">()</span>
<span class="n">rc</span><span class="p">(</span><span class="s">'font'</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="n">font_name</span><span class="p">)</span>
<span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s">'figure.autolayout'</span><span class="p">:</span> <span class="bp">True</span><span class="p">})</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c"># Limit number of tokens to be visualized</span>
<span class="n">limit</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">vector_dim</span> <span class="o">=</span> <span class="mi">300</span>

<span class="c"># Getting tokens and vectors</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">ko_model</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
    <span class="c"># Break the loop if limit exceeds </span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">limit</span><span class="p">:</span> <span class="k">break</span>

    <span class="c"># Getting token </span>
    <span class="n">words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

    <span class="c"># Appending the vectors </span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">ko_model</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>

    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c"># Reshaping the embedding vector </span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">limit</span><span class="p">,</span> <span class="n">vector_dim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_with_labels</span><span class="p">(</span><span class="n">low_dim_embs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s">'tsne.png'</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">low_dim_embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="s">"More labels than embeddings"</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">18</span><span class="p">))</span>  <span class="c"># in inches</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">low_dim_embs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span>
                 <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
                 <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
                 <span class="n">textcoords</span><span class="o">=</span><span class="s">'offset points'</span><span class="p">,</span>
                 <span class="n">ha</span><span class="o">=</span><span class="s">'right'</span><span class="p">,</span>
                 <span class="n">va</span><span class="o">=</span><span class="s">'bottom'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>


<span class="c"># Creating the tsne plot [Warning: will take time]</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mf">30.0</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">'pca'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

<span class="n">low_dim_embedding</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>

<span class="c"># Finally plotting and saving the fig </span>
<span class="n">plot_with_labels</span><span class="p">(</span><span class="n">low_dim_embedding</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/home/younkyung.jang/miniconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  warnings.warn("This figure includes Axes that are not compatible "
</code></pre></div></div>

<p><img src="/assets/img/output_21_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">similarities</span> <span class="o">=</span> <span class="n">ko_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s">'동물'</span><span class="p">,</span> <span class="s">'파충류'</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s">'뱀'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">similarities</span><span class="p">)</span>

<span class="n">not_matching</span> <span class="o">=</span> <span class="n">ko_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">doesnt_match</span><span class="p">(</span><span class="s">"아침 점심 저녁 된장국"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">not_matching</span><span class="p">)</span>

<span class="n">sim_score</span> <span class="o">=</span> <span class="n">ko_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">'컴퓨터'</span><span class="p">,</span> <span class="s">'인간'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sim_score</span><span class="p">)</span>

<span class="n">sim_score</span> <span class="o">=</span> <span class="n">ko_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">'로봇'</span><span class="p">,</span> <span class="s">'인간'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sim_score</span><span class="p">)</span>

<span class="n">sim_score</span> <span class="o">=</span> <span class="n">ko_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">'사랑해'</span><span class="p">,</span> <span class="s">'사랑의'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sim_score</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">ko_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s">'전자'</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[('포유류', 0.7234190702438354), ('포유동물', 0.7082793712615967), ('절지동물', 0.6905428171157837), ('양서류', 0.6887608766555786), ('독동물', 0.6857677698135376), ('포유류분류', 0.6800143718719482), ('무척추동물', 0.6791884899139404), ('척추동물분류', 0.6789263486862183), ('도시동물', 0.6775411367416382), ('동물상', 0.6730656623840332)]
된장국
0.42482013475966973
0.4782262080863405
0.5480147143801912
[('전자빔', 0.7568391561508179), ('전자렌지', 0.7566049098968506), ('전자기기', 0.7503113746643066), ('전자양', 0.7480576634407043), ('가전자', 0.7460261583328247), ('전자악기', 0.7431635856628418), ('전자기기와', 0.7412865161895752), ('전자기계', 0.7339802980422974), ('전자만', 0.7331153154373169), ('전자적인', 0.7289555072784424)]


/home/younkyung.jang/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).
  """Entry point for launching an IPython kernel.
/home/younkyung.jang/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).
  after removing the cwd from sys.path.
/home/younkyung.jang/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).
  import sys
/home/younkyung.jang/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).
  # Remove the CWD from sys.path while we load stuff.
/home/younkyung.jang/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).
  del sys.path[0]
/home/younkyung.jang/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).
  app.launch_new_instance()
</code></pre></div></div>

<h3 id="2-self-trained-fasttext-by-fasttext-api">2. Self-trained FastText by FastText API</h3>
<p>https://github.com/facebookresearch/fastText</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git clone https://github.com/facebookresearch/fastText.git
$ cd fastText
$ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..
$ make &amp;&amp; make install
</code></pre></div></div>

<p><strong>fastText 내의 word-vector-example.sh / classification-example.sh 파일 참고</strong></p>
<ul>
  <li>word vector 만들기<br />
./fasttext skipgram -input input.txt -lr 0.025 -dim 100 -ws 5 -epoch 1 -minCount 5 -neg 5 -loss ns -bucket 2000000 -minn 3 -maxn 6 -thread 4 -t 1e-4 -lrUpdateRate 100<br />
<br /></li>
  <li>classification 모델 만들기<br />
./fasttext supervised -input input.txt -output “output.bin” -dim 10 -lr 0.1 -wordNgrams 2 -minCount 1 -bucket 10000000 -epoch 5 -thread 4</li>
</ul>

<p><strong>Similar Words 계산</strong></p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ : ./fasttext nn model.bin  
</code></pre></div></div>
<p><img src="/assets/img/ft1.png" height="300" width="600" /></p>

<p><strong>classificatoin 모델로 예측하기</strong></p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ./fasttext test "model.bin" "test.txt"
$ ./fasttext supervised -input cooking.train -output model_cooking -lr 1.0 -epoch 25 -wordNgrams 2
$ ./fasttext test model_cooking.bin cooking.valid
</code></pre></div></div>
<ul>
  <li>changing the number of epochs (using the option -epoch, standard range (5 - 50)</li>
  <li>changing the learning rate (using the option -lr, standard range (0.1 - 1.0)</li>
  <li>using word n-grams (using the option -wordNgrams, standard range (1 - 5)</li>
</ul>

<h3 id="3-self-trained-fasttext-by-gensim">3. Self-trained FastText by Gensim</h3>
<p>참고 : https://radimrehurek.com/gensim/models/fasttext.html<br />
파이썬 Gensim 패키지를 통해서 fastText 모델을 만들 수 있다.<br />
Gensim 패키지를 이용하면 fastText 모델을 word2vec format으로 변형해서 로드할 수 있어서 기존 word2vec api를 사용할 수도 있고, 다른 모델링(deep learning 등)의 input 형태로 변환하기도 수월해진다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">gensim.test.utils</span> <span class="kn">import</span> <span class="n">common_texts</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">FastText</span>

<span class="n">ft_model</span> <span class="o">=</span> <span class="n">FastText</span><span class="p">(</span><span class="n">common_texts</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">common_texts</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>

<span class="n">similarities</span> <span class="o">=</span> <span class="n">ft_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s">'computer'</span><span class="p">,</span> <span class="s">'human'</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s">'interface'</span><span class="p">])</span>
<span class="n">most_similar</span> <span class="o">=</span> <span class="n">similarities</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">most_similar</span><span class="p">)</span>

<span class="n">not_matching</span> <span class="o">=</span> <span class="n">ft_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">doesnt_match</span><span class="p">(</span><span class="s">"human computer interface tree"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">not_matching</span><span class="p">)</span>

<span class="n">sim_score</span> <span class="o">=</span> <span class="n">ft_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">'computer'</span><span class="p">,</span> <span class="s">'human'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sim_score</span><span class="p">)</span>

<span class="n">sim_score</span> <span class="o">=</span> <span class="n">ft_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">'computer'</span><span class="p">,</span> <span class="s">'interface'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sim_score</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>('graph', 0.8366204500198364)
tree
-0.13334978210321657
0.003142370718195925
</code></pre></div></div>

<h3 id="내가-가진-데이터로-fasttext-모델을-만들어보기">내가 가진 데이터로 FastText 모델을 만들어보기</h3>

<p><strong>Self-trained FastText</strong></p>
<ul>
  <li>데이터 : 한국어 crawling data 41만건</li>
  <li>Parameter : skipgram , -lr 0.025 , -dim 300 , -ws 5 , -epoch 20 , -lrUpdateRate 100</li>
</ul>

<p><strong>The following arguments for the dictionary are optional:</strong></p>
<ul>
  <li>-minCount : minimal number of word occurences</li>
  <li>-minCountLabel : minimal number of label occurences</li>
  <li>-wordNgrams : max length of word ngram</li>
  <li>-bucket : number of buckets</li>
  <li>-minn : min length of char ngram</li>
  <li>-maxn : max length of char ngram</li>
  <li>-t : sampling threshold</li>
  <li>-label : labels prefix</li>
</ul>

<p><strong>The following arguments for training are optional:</strong></p>
<ul>
  <li>-lr : learning rate</li>
  <li>-lrUpdateRate : change the rate of updates for the learning rate</li>
  <li>-dim : size of word vectors</li>
  <li>-ws : size of the context window</li>
  <li>-epoch : number of epochs</li>
  <li>-neg : number of negatives sampled</li>
  <li>-loss : loss function {ns, hs, softmax}</li>
  <li>-thread : number of threads</li>
  <li>-pretrainedVectors : pretrained word vectors for supervised learning</li>
  <li>-saveOutput : whether output params should be saved</li>
</ul>

<p>self-trained fastText with Crawling Data
<img src="/assets/img/ft2.png" height="300" width="600" /></p>

<h3 id="word2vec-vs-fasttext">Word2Vec VS FastText</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gensim</span>

<span class="c"># Creating the model</span>
<span class="n">ko_w2v</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'ko.bin'</span><span class="p">)</span> <span class="c"># load word2vec model</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">ko_w2v</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s">"전자"</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="n">similarities_wv</span> <span class="o">=</span> <span class="n">ko_w2v</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s">'동물'</span><span class="p">,</span> <span class="s">'파충류'</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s">'뱀'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">similarities_wv</span><span class="p">)</span>

<span class="n">sim_score_wv</span> <span class="o">=</span> <span class="n">ko_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">'컴퓨터'</span><span class="p">,</span> <span class="s">'인간'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sim_score_wv</span><span class="p">)</span>

<span class="n">sim_score_wv</span> <span class="o">=</span> <span class="n">ko_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s">'로봇'</span><span class="p">,</span> <span class="s">'인간'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sim_score_wv</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[('반도체', 0.6502741575241089), ('양전자', 0.6052197217941284), ('복사기', 0.5808517336845398), ('음전하', 0.5768587589263916), ('원자가', 0.5756815671920776), ('음극', 0.5747135281562805), ('양전하', 0.5658353567123413), ('절연체', 0.5621837377548218), ('상거래', 0.5594459772109985), ('광자', 0.5468275547027588)]
[('생물', 0.6952868700027466), ('영장류', 0.6766470670700073), ('조류', 0.6660945415496826), ('양서류', 0.6637342572212219), ('포유류', 0.659113347530365), ('설치류', 0.636635422706604), ('무척추', 0.6241835355758667), ('어류', 0.6236225366592407), ('절지', 0.6208628416061401), ('곤충', 0.6167744398117065)]
0.42482013475966973
0.4782262080863405


/home/younkyung.jang/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).
  """Entry point for launching an IPython kernel.
/home/younkyung.jang/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).
  This is separate from the ipykernel package so we can avoid doing imports until
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c"># Limit number of tokens to be visualized</span>
<span class="n">limit</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">vector_dim</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c"># Getting tokens and vectors</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">ko_w2v</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
    <span class="c"># Break the loop if limit exceeds </span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">limit</span><span class="p">:</span> <span class="k">break</span>

    <span class="c"># Getting token </span>
    <span class="n">words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

    <span class="c"># Appending the vectors </span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">ko_w2v</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>

    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c"># Reshaping the embedding vector </span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">limit</span><span class="p">,</span> <span class="n">vector_dim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_with_labels</span><span class="p">(</span><span class="n">low_dim_embs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s">'tsne.png'</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">low_dim_embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="s">"More labels than embeddings"</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">18</span><span class="p">))</span>  <span class="c"># in inches</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">low_dim_embs</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span>
                 <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
                 <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                 <span class="n">textcoords</span><span class="o">=</span><span class="s">'offset points'</span><span class="p">,</span>
                 <span class="n">ha</span><span class="o">=</span><span class="s">'right'</span><span class="p">,</span>
                 <span class="n">va</span><span class="o">=</span><span class="s">'bottom'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>


<span class="c"># Creating the tsne plot [Warning: will take time]</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">perplexity</span><span class="o">=</span><span class="mf">30.0</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">'pca'</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

<span class="n">low_dim_embedding</span> <span class="o">=</span> <span class="n">tsne</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>

<span class="c"># Finally plotting and saving the fig </span>
<span class="n">plot_with_labels</span><span class="p">(</span><span class="n">low_dim_embedding</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/home/younkyung.jang/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).
/home/younkyung.jang/miniconda3/lib/python3.6/site-packages/matplotlib/figure.py:2267: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
  warnings.warn("This figure includes Axes that are not compatible "
</code></pre></div></div>

<p><img src="/assets/img/output_32_1.png" alt="png" /></p>

<h1 id="in-practice">In Practice</h1>

<p><strong>장점</strong></p>
<ul>
  <li>노이즈가 많은 댓글 데이터에서는 word2vec보다 fastText의 성능이 더 좋았음</li>
  <li>Word2vec의 경우 오타 또는 줄임말 경우에 out of vocabulary 문제가 생김 ex) 재미업다. 잇어요..</li>
</ul>

<p><strong>단점</strong></p>
<ul>
  <li>distribution hyopthesis 기반으로 학습된 모델이기 때문에 의미상 아무런 관련이 없는 단어임에도 벡터 공간이 가깝게 임베딩 됨</li>
  <li>
    <p>과일 가게에 –를 사러 갔다. =&gt; 사과, 바나나, 귤, 복숭아 등</p>
  </li>
  <li>사과, 바나나, 귤, 복숭아 등은 문맥상으로는 다른 단어이지만 문법적으로 근접하다면 similarity가 높은 유사도로 계산되어 진다.</li>
  <li>따라서 카테고리 사전, topic clustering으로 할 때 완전 자동화 할 수는 없고 filtering 수작업이 필요하다.</li>
</ul>

<h2 id="rerefence">Rerefence</h2>
<p>https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/11/embedding/<br />
https://www.lucypark.kr/docs/2015-pyconkr/#37<br />
https://www.mathworks.com/help/textanalytics/examples/visualize-word-embedding-using-text-scatter-plot.html<br />
https://blog.webhose.io/2017/02/09/how-to-use-rated-reviews-for-sentiment-classification/</p>


        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#fasttext" class="page__taxonomy-item" rel="tag">fasttext</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#python" class="page__taxonomy-item" rel="tag">Python</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#word-embedding" class="page__taxonomy-item" rel="tag">word embedding</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#word2vec" class="page__taxonomy-item" rel="tag">word2vec</a>
    
    </span>
  </p>




  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/categories/#data-analysis" class="page__taxonomy-item" rel="tag">Data Analysis</a>
    
    </span>
  </p>


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time datetime="2018-08-29T00:00:00+09:00">August 29, 2018</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/writing/miracle_morning/" class="pagination--pager" title="미라클 모닝
">이전</a>
    
    
      <a href="/writing/cosmos/" class="pagination--pager" title="코스모스, 칼 세이건
">다음</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">참고</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/kaggle_kernel/" rel="permalink">Kaggle_kernel
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">
</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/writing/sapiens_nongup/" rel="permalink">사피엔스 필사2_농업혁명
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">사피엔스 제 2장. 농업혁명 관련 필사
(손 필사… 힘들다… ;;)
  












</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/writing/thougts/" rel="permalink">감옥으로부터의 사색 중에서
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">
  없는 사람이 살기는 겨울보다 여름이 낫다고 하지만 교도소의 우리들은 없이 살기는 더합니다만 차라리 겨울을 택합니다. 왜냐하면 여름 징역의 열 가지 스무 가지 장점을 일시에 무색케 해버리는 결정적인 사실 ― 여름 징역은 자기의 바로 옆사람을 증오하게 한다는 사실 때문입니다.

...</p>
  </article>
</div>
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/data%20analysis/Toxic_Preprocessing/" rel="permalink">Kaggle Toxic Classification (2) Preprocessing
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">Toxic Comment Classification Kaggle 대회 : 링크

</p>
  </article>
</div>
        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap">
  <input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  <div id="results" class="results"></div>
</div>
      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우:</strong></li>
    
    
    
    
      <li><a href="https://github.com/InspiringPeople"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
	
      <li><a href="https://youtube.com/dearpiano"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i> youtube</a></li>
    
    
    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 YounKyung Jang. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>


      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://use.fontawesome.com/releases/v5.1.0/js/all.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-121538180-1']);
  
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>





    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/data%20analysis/word_embedding/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/data%20analysis/word_embedding"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://https-inspiringpeople-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  



  </body>
</html>